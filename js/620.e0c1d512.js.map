{"version":3,"file":"js/620.e0c1d512.js","mappings":"mJAAO,MAAMA,EAA4B,CACrC,gCAAAC,CAAiCC,GAC7B,MAAO,uDAAuDA,EAAMC,iCACxE,EACA,gCAAAC,CAAiCC,EAAUC,EAAaC,EAAQC,EAAMC,GAClE,MAAQ,2BAA2BJ,EAASK,OAAOJ,mBAA6BA,cAA6BC,eACjH,E,8UCLJ,IAAII,EAAiB,CAAC,EACtB,MAAMC,EAAe,IAAI,IAClB,SAASC,EAAaC,GACzB,MAAMC,EAAYD,EAAOE,WACzB,GAAIL,EAAeM,eAAeF,GAC9B,OAAOJ,EAAeI,GAErB,CACD,MAAMG,EAAYN,EAAaO,QAAQJ,GAEvC,OADAJ,EAAeI,GAAaG,EACrBA,CACX,CACJ,CACO,SAASE,IACZT,EAAiB,CAAC,CACtB,CCXA,MAAMU,EAAyB,gEAClBC,EAA8B,oDACpC,SAASC,EAA8BT,EAAQU,GAAsB,GACxE,IACI,MAAMC,EAAMZ,EAAaC,GACnBY,EAAaC,EAA0BF,EAAIG,MAAO,CAAC,EAAGH,EAAII,MAAMC,YACtE,OAAOJ,CACX,CACA,MAAOK,GAIH,GAAIA,EAAEC,UAAYX,EACVG,IACA,OAAc,GAAGF,4BACcR,EAAOE,kOAMzC,CACD,IAAIiB,EAAY,GACZT,IACAS,EACI,iKAGR,OAAY,GAAGX,0BACYR,EAAOE,0IAG9BiB,EACR,CACJ,CACA,MAAO,EACX,CACO,SAASN,EAA0BF,EAAKS,EAAQJ,GACnD,OAAQL,EAAIU,MACR,IAAK,cACD,IAAK,IAAIC,EAAI,EAAGA,EAAIX,EAAIG,MAAMrB,OAAQ6B,IAClCT,EAA0BF,EAAIG,MAAMQ,GAAIF,EAAQJ,GAEpD,MACJ,IAAK,cACD,MAAMO,EAAQZ,EAAIG,MAClB,IAAK,IAAIQ,EAAI,EAAGA,EAAIC,EAAM9B,OAAQ6B,IAAK,CACnC,MAAME,EAAOD,EAAMD,GAEnB,OAAQE,EAAKH,MACT,IAAK,YAIL,IAAK,qBAEL,IAAK,YACL,IAAK,oBACL,IAAK,cACL,IAAK,eACL,IAAK,kBACD,SAER,MAAMI,EAAOD,EACb,OAAQC,EAAKJ,MACT,IAAK,YACDK,EAAwBD,EAAKX,MAAOM,EAAQJ,GAC5C,MACJ,IAAK,MACD,IAAwB,IAApBS,EAAKE,WACL,MAAMC,MAAMrB,IAEhB,EAAAsB,EAAA,GAAQJ,EAAKX,OAAQgB,IACjB,GAAoB,kBAATA,EACPJ,EAAwBI,EAAMV,EAAQJ,OAErC,CAED,MAAMe,EAAQD,EAEd,IAAmB,IAAfd,EACA,IAAK,IAAIgB,EAAYD,EAAME,KAAMD,GAAaD,EAAMG,GAAIF,IACpDN,EAAwBM,EAAWZ,EAAQJ,OAI9C,CAED,IAAK,IAAIgB,EAAYD,EAAME,KAAMD,GAAaD,EAAMG,IAAMF,EAAYG,GAAoBH,IACtFN,EAAwBM,EAAWZ,EAAQJ,GAG/C,GAAIe,EAAMG,IAAMC,GAAoB,CAChC,MAAMC,EAAcL,EAAME,MAAQE,GAC5BJ,EAAME,KACNE,GACAE,EAAcN,EAAMG,GACpBI,EAAYC,GAAyBH,GACrCI,EAAYD,GAAyBF,GAC3C,IAAK,IAAII,EAAaH,EAAWG,GAAcD,EAAWC,IACtDrB,EAAOqB,GAAcA,CAE7B,CACJ,CACJ,KAEJ,MACJ,IAAK,QACD5B,EAA0BY,EAAKX,MAAOM,EAAQJ,GAC9C,MAEJ,QACI,MAAMY,MAAM,wBAGpB,MAAMc,OAA2CC,IAApBlB,EAAKmB,YAAwD,IAA5BnB,EAAKmB,WAAWC,QAC9E,GAGe,UAAdpB,EAAKJ,OAA8C,IAA1ByB,EAAgBrB,IAEvB,UAAdA,EAAKJ,OAA6C,IAAzBqB,EAC1B,KAER,CACA,MAEJ,QACI,MAAMd,MAAM,yBAGpB,OAAO,EAAAmB,EAAA,GAAO3B,EAClB,CACA,SAASM,EAAwBI,EAAMV,EAAQJ,GAC3C,MAAMgC,EAAmBT,GAAyBT,GAClDV,EAAO4B,GAAoBA,GACR,IAAfhC,GACAiC,EAAiBnB,EAAMV,EAE/B,CACA,SAAS6B,EAAiBnB,EAAMV,GAC5B,MAAM8B,EAAOC,OAAOC,aAAatB,GAC3BuB,EAAYH,EAAKI,cAEvB,GAAID,IAAcH,EAAM,CACpB,MAAMF,EAAmBT,GAAyBc,EAAUE,WAAW,IACvEnC,EAAO4B,GAAoBA,CAC/B,KACK,CACD,MAAMQ,EAAYN,EAAKO,cACvB,GAAID,IAAcN,EAAM,CACpB,MAAMF,EAAmBT,GAAyBiB,EAAUD,WAAW,IACvEnC,EAAO4B,GAAoBA,CAC/B,CACJ,CACJ,CACA,SAASU,EAASC,EAASC,GACvB,OAAO,EAAAC,EAAA,GAAKF,EAAQ7C,OAAQgD,IACxB,GAA2B,kBAAhBA,EACP,OAAO,EAAAC,EAAA,GAASH,EAAiBE,GAEhC,CAED,MAAM/B,EAAQ+B,EACd,YAAsGnB,KAA9F,EAAAkB,EAAA,GAAKD,GAAkBI,GAAejC,EAAME,MAAQ+B,GAAcA,GAAcjC,EAAMG,IAClG,IAER,CACA,SAASY,EAAgBnC,GACrB,MAAMiC,EAAajC,EAAIiC,WACvB,SAAIA,GAAqC,IAAvBA,EAAWC,YAGxBlC,EAAIG,SAGF,EAAAmD,EAAA,GAAQtD,EAAIG,QACb,EAAAoD,EAAA,GAAMvD,EAAIG,MAAOgC,GACjBA,EAAgBnC,EAAIG,OAC9B,CACA,MAAMqD,UAAuB,IACzB,WAAAC,CAAYR,GACRS,QACAC,KAAKV,gBAAkBA,EACvBU,KAAKC,OAAQ,CACjB,CACA,aAAAC,CAAcC,GAEV,IAAmB,IAAfH,KAAKC,MAAT,CAKA,OAAQE,EAAKpD,MACT,IAAK,YAED,YADAiD,KAAKI,eAAeD,GAExB,IAAK,oBAED,YADAH,KAAKK,uBAAuBF,GAGpCJ,MAAMG,cAAcC,EAXpB,CAYJ,CACA,cAAAG,CAAeH,IACP,EAAAV,EAAA,GAASO,KAAKV,gBAAiBa,EAAK3D,SACpCwD,KAAKC,OAAQ,EAErB,CACA,QAAAM,CAASJ,GACDA,EAAK9C,gBACwCgB,IAAzCe,EAASe,EAAMH,KAAKV,mBACpBU,KAAKC,OAAQ,QAI4B5B,IAAzCe,EAASe,EAAMH,KAAKV,mBACpBU,KAAKC,OAAQ,EAGzB,EAEG,SAASO,EAAiBC,EAAW1E,GACxC,GAAIA,aAAmB2E,OAAQ,CAC3B,MAAMrE,EAAMZ,EAAaM,GACnB4E,EAAiB,IAAId,EAAeY,GAE1C,OADAE,EAAeC,MAAMvE,GACdsE,EAAeV,KAC1B,CAEI,YAEO5B,KAFC,EAAAkB,EAAA,GAAKxD,GAAU6C,IACZ,EAAAa,EAAA,GAASgB,EAAW7B,EAAKK,WAAW,KAGvD,CCxOA,MAAM4B,EAAU,UACHC,EAAe,cACfC,EAAQ,QACd,IAAIC,EAAsD,mBAA9B,IAAIN,OAAO,QAAQO,OAO/C,SAASC,EAAkBC,EAAYC,GAC1CA,GAAU,EAAAC,EAAA,GAASD,EAAS,CACxBE,UAAWN,EACXO,OAAO,EACPC,UAAU,EACVC,iBAAkB,OAClBC,yBAA0B,CAAC,KAAM,MACjCC,OAAQ,CAACC,EAAKC,IAAWA,MAE7B,MAAMF,EAASP,EAAQO,OAIvB,IAAIG,EAHJH,EAAO,mCAAmC,KACtCI,IAAiC,IAGrCJ,EAAO,mBAAmB,KACtBG,GAAoB,EAAAE,EAAA,GAAOb,GAAac,GAC7BA,EAASpB,KAAaqB,GAAMC,IACrC,IAEN,IACIC,EA2EAC,EACAC,EACAC,EACAC,EACAC,EA+BAC,EAgBAC,EACAC,EACAC,EACAC,EAlIAC,GAAY,EAEhBpB,EAAO,sBAAsB,KACzBoB,GAAY,EACZX,GAAyB,EAAAY,EAAA,GAAIlB,GAAoBG,IAC7C,MAAMgB,EAAchB,EAASpB,GAE7B,IAAI,EAAAqC,EAAA,GAASD,GAAc,CACvB,MAAME,EAAeF,EAAYG,OACjC,OAA4B,IAAxBD,EAAahI,QAEI,MAAjBgI,GACiB,MAAjBA,GACiB,MAAjBA,GACCF,EAAYvG,WAGgB,IAAxByG,EAAahI,QACE,OAApBgI,EAAa,KAEZ,EAAA1D,EAAA,GAAS,CACN,IACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,KACD0D,EAAa,IAOT/B,EAAQE,UACT+B,GAAcJ,GACdK,GAAgBL,GALfE,EAAa,GA1BbA,CAiCf,CACK,IAAI,EAAAI,EAAA,GAAWN,GAGhB,OAFAF,GAAY,EAEL,CAAES,KAAMP,GAEd,GAA2B,kBAAhBA,EAGZ,OAFAF,GAAY,EAELE,EAEN,GAA2B,kBAAhBA,EAA0B,CACtC,GAA2B,IAAvBA,EAAY9H,OACZ,OAAO8H,EAEN,CACD,MAAMQ,EAAsBR,EAAYS,QAAQ,sBAAuB,QACjEC,EAAgB,IAAIjD,OAAO+C,GACjC,OAAOrC,EAAQE,UACT+B,GAAcM,GACdL,GAAgBK,EAC1B,CACJ,CAEI,MAAMrG,MAAM,uBAChB,GACF,IAONqE,EAAO,gBAAgB,KACnBU,GAAmB,EAAAW,EAAA,GAAIlB,GAAoBG,GAAaA,EAAS2B,eACjEtB,GAAoB,EAAAU,EAAA,GAAIlB,GAAoB+B,IACxC,MAAMC,EAAYD,EAAME,MAExB,GAAID,IAAc5B,GAAM8B,QAAxB,CAGK,IAAI,EAAAC,EAAA,GAASH,GACd,OAAOA,EAEN,IAAI,EAAAI,EAAA,GAAYJ,GACjB,OAAO,EAGP,MAAMxG,MAAM,uBAChB,KAEJiF,GAA8B,EAAAS,EAAA,GAAIlB,GAAoB+B,IAClD,MAAMM,EAAgBN,EAAMO,WAC5B,GAAID,EAAe,CACf,MAAME,GAAkB,EAAA1E,EAAA,GAAQwE,IAC1B,EAAAnB,EAAA,GAAImB,GAAgBpH,IAAS,EAAAuH,EAAA,GAAQxC,EAAmB/E,KACxD,EAAC,EAAAuH,EAAA,GAAQxC,EAAmBqC,IAClC,OAAOE,CACX,KAEJ7B,GAAuB,EAAAQ,EAAA,GAAIlB,GAAoB+B,GAAUA,EAAMU,YAC/D9B,GAAsB,EAAAO,EAAA,GAAIlB,GAAoB+B,IAAU,EAAAW,EAAA,GAAIX,EAAO,aAAY,IAGnFlC,EAAO,4BAA4B,KAC/B,MAAM8C,EAA0BC,GAAatD,EAAQM,0BACrDgB,GAAgC,EAAAM,EAAA,GAAIlB,GAAoB6C,IAAY,IACnC,eAA7BvD,EAAQK,mBACRiB,GAAgC,EAAAM,EAAA,GAAIlB,GAAoB6C,IAChD,EAAAH,EAAA,GAAIG,EAAS,iBACJA,EAAQC,aAGmD,IAA5DC,GAAsBF,EAASF,IACnCjE,EAAiBiE,EAAyBE,EAAQ9D,WAGlE,IAMJc,EAAO,mBAAmB,KACtBgB,GAAuB,EAAAK,EAAA,GAAIlB,EAAmBgD,IAC9ClC,GAAoB,EAAAI,EAAA,GAAIZ,EAAwB2C,IAChDlC,GAAc,EAAAmC,EAAA,GAAOlD,GAAmB,CAACmD,EAAKpB,KAC1C,MAAMC,EAAYD,EAAME,MAIxB,OAHI,EAAAE,EAAA,GAASH,IAAgBA,IAAc5B,GAAM8B,UAC7CiB,EAAInB,GAAa,IAEdmB,CAAG,GACX,CAAC,GACJnC,GAAqB,EAAAE,EAAA,GAAIZ,GAAwB,CAAC8C,EAAGC,KAC1C,CACHpJ,QAASqG,EAAuB+C,GAChCC,UAAW7C,EAA4B4C,GACvCE,kBAAmB3C,EAA8ByC,GACjDG,SAAU3C,EAAqBwC,GAC/BI,MAAO3C,EAAkBuC,GACzBK,MAAOlD,EAAkB6C,GACzBM,KAAMjD,EAAqB2C,GAC3BO,IAAKjD,EAAoB0C,GACzBvB,aAAcvB,EAAiB8C,GAC/BQ,UAAW7D,EAAkBqD,MAEnC,IAEN,IAAIS,GAAiB,EACjBC,EAA+B,GAkEnC,OAjEKzE,EAAQI,UACTG,EAAO,2BAA2B,KAC9BkE,GAA+B,EAAAb,EAAA,GAAOlD,GAAmB,CAAChF,EAAQgJ,EAAaX,KAC3E,GAAmC,kBAAxBW,EAAYjF,QAAsB,CACzC,MAAMkF,EAAWD,EAAYjF,QAAQ5B,WAAW,GAC1C+G,EAAe/H,GAAyB8H,GAC9CE,GAAiBnJ,EAAQkJ,EAAclD,EAAmBqC,GAC9D,MACK,IAAI,EAAAxF,EAAA,GAAQmG,EAAYI,kBAAmB,CAC5C,IAAIC,GACJ,EAAA5I,EAAA,GAAQuI,EAAYI,kBAAmBE,IACnC,MAAML,EAAgC,kBAAdK,EAClBA,EAAUnH,WAAW,GACrBmH,EACAC,EAAmBpI,GAAyB8H,GAK9CI,IAAqBE,IACrBF,EAAmBE,EACnBJ,GAAiBnJ,EAAQuJ,EAAkBvD,EAAmBqC,IAClE,GAER,MACK,IAAI,EAAAjC,EAAA,GAAS4C,EAAYjF,SAC1B,GAAIiF,EAAYjF,QAAQyF,QACpBV,GAAiB,EACbxE,EAAQhF,sBACR,OAAY,GAAGF,0BACc4J,EAAYjF,QAAQjF,8QAMpD,CACD,MAAM2K,EAAiBpK,EAA8B2J,EAAYjF,QAASO,EAAQhF,sBAI9E,EAAAoK,EAAA,GAAQD,KAIRX,GAAiB,IAErB,EAAArI,EAAA,GAAQgJ,GAAiB/I,IACrByI,GAAiBnJ,EAAQU,EAAMsF,EAAmBqC,GAAK,GAE/D,MAGI/D,EAAQhF,sBACR,OAAY,GAAGF,kBACM4J,EAAYW,qPAIrCb,GAAiB,EAErB,OAAO9I,CAAM,GACd,GAAG,IAGP,CACH+F,YAAaA,EACbC,mBAAoBA,EACpB+C,6BAA8BA,EAC9B9C,UAAWA,EACX6C,eAAgBA,EAExB,CACO,SAASc,EAAiBvF,EAAYwF,GACzC,IAAIC,EAAS,GACb,MAAMC,EAAgBC,EAAoB3F,GAC1CyF,EAASA,EAAOG,OAAOF,EAAcD,QACrC,MAAMI,EAAgBC,EAAoBJ,EAAcK,OAClDC,EAAkBH,EAAcE,MAMtC,OALAN,EAASA,EAAOG,OAAOC,EAAcJ,QACrCA,EAASA,EAAOG,OAAOK,EAAsBD,IAC7CP,EAASA,EAAOG,OAAOM,GAAqBF,IAC5CP,EAASA,EAAOG,OAAOO,GAAwBH,EAAiBR,IAChEC,EAASA,EAAOG,OAAOQ,GAAwBJ,IACxCP,CACX,CACA,SAASQ,EAAsBjG,GAC3B,IAAIyF,EAAS,GACb,MAAMY,GAAqB,EAAAC,EAAA,GAAOtG,GAAa2E,IAAgB,EAAA5C,EAAA,GAAS4C,EAAYjF,MAMpF,OALA+F,EAASA,EAAOG,OAAOW,EAAqBF,IAC5CZ,EAASA,EAAOG,OAAOY,GAAuBH,IAC9CZ,EAASA,EAAOG,OAAOa,GAAqBJ,IAC5CZ,EAASA,EAAOG,OAAOc,GAAsBL,IAC7CZ,EAASA,EAAOG,OAAOe,EAAsBN,IACtCZ,CACX,CACO,SAASE,EAAoB3F,GAChC,MAAM4G,GAA+B,EAAAN,EAAA,GAAOtG,GAAac,KAC7C,EAAAuC,EAAA,GAAIvC,EAAUpB,KAEpB+F,GAAS,EAAA5D,EAAA,GAAI+E,GAA+B9F,IACvC,CACHrF,QAAS,iBACLqF,EAASwE,KACT,uCACJ1J,KAAMiL,GAAyBC,gBAC/B9G,WAAY,CAACc,OAGfiF,GAAQ,EAAAgB,EAAA,GAAW/G,EAAY4G,GACrC,MAAO,CAAEnB,SAAQM,QACrB,CACO,SAASD,EAAoB9F,GAChC,MAAMgH,GAA+B,EAAAV,EAAA,GAAOtG,GAAac,IACrD,MAAMlG,EAAUkG,EAASpB,GACzB,QAAS,EAAAqC,EAAA,GAASnH,MACb,EAAAwH,EAAA,GAAWxH,MACX,EAAAyI,EAAA,GAAIzI,EAAS,WACb,EAAAkI,EAAA,GAASlI,EAAS,IAErB6K,GAAS,EAAA5D,EAAA,GAAImF,GAA+BlG,IACvC,CACHrF,QAAS,iBACLqF,EAASwE,KADJ,0JAIT1J,KAAMiL,GAAyBI,gBAC/BjH,WAAY,CAACc,OAGfiF,GAAQ,EAAAgB,EAAA,GAAW/G,EAAYgH,GACrC,MAAO,CAAEvB,SAAQM,QACrB,CACA,MAAMmB,EAAe,WACd,SAASX,EAAqBvG,GACjC,MAAMmH,UAAwB,IAC1B,WAAAxI,GACIC,SAASwI,WACTvI,KAAKC,OAAQ,CACjB,CACA,cAAAuI,CAAerI,GACXH,KAAKC,OAAQ,CACjB,EAEJ,MAAMwI,GAAe,EAAAhB,EAAA,GAAOtG,GAAac,IACrC,MAAMlG,EAAUkG,EAASpB,QACzB,IACI,MAAM6H,EAAYjN,EAAaM,GACzB4M,EAAmB,IAAIL,EAE7B,OADAK,EAAiB/H,MAAM8H,GAChBC,EAAiB1I,KAC5B,CACA,MAAOtD,GAGH,OAAO0L,EAAaO,KAAK7M,EAAQqH,OACrC,KAEEwD,GAAS,EAAA5D,EAAA,GAAIyF,GAAexG,IACvB,CACHrF,QAAS,oDAELqF,EAASwE,KAFJ,+IAMT1J,KAAMiL,GAAyBa,iBAC/B1H,WAAY,CAACc,OAGrB,OAAO2E,CACX,CACO,SAASkB,EAAsB3G,GAClC,MAAM2H,GAAqB,EAAArB,EAAA,GAAOtG,GAAac,IAC3C,MAAMlG,EAAUkG,EAASpB,QACzB,OAAO9E,EAAQ6M,KAAK,GAAG,IAErBhC,GAAS,EAAA5D,EAAA,GAAI8F,GAAqB7G,IAC7B,CACHrF,QAAS,iBACLqF,EAASwE,KACT,qDACJ1J,KAAMiL,GAAyBe,oBAC/B5H,WAAY,CAACc,OAGrB,OAAO2E,CACX,CACA,MAAMoC,GAAiB,iBAChB,SAASrB,GAAuBxG,GACnC,MAAM8H,UAA0B,IAC5B,WAAAnJ,GACIC,SAASwI,WACTvI,KAAKC,OAAQ,CACjB,CACA,gBAAAiJ,CAAiB/I,GACbH,KAAKC,OAAQ,CACjB,EAEJ,MAAMwI,GAAe,EAAAhB,EAAA,GAAOtG,GAAac,IACrC,MAAMlG,EAAUkG,EAASpB,QACzB,IACI,MAAM6H,EAAYjN,EAAaM,GACzBoN,EAAqB,IAAIF,EAE/B,OADAE,EAAmBvI,MAAM8H,GAClBS,EAAmBlJ,KAC9B,CACA,MAAOtD,GAGH,OAAOqM,GAAeJ,KAAK7M,EAAQqH,OACvC,KAEEwD,GAAS,EAAA5D,EAAA,GAAIyF,GAAexG,IACvB,CACHrF,QAAS,oDAELqF,EAASwE,KAFJ,yJAMT1J,KAAMiL,GAAyBoB,iBAC/BjI,WAAY,CAACc,OAGrB,OAAO2E,CACX,CACO,SAASgB,GAAqBzG,GACjC,MAAMkI,GAAe,EAAA5B,EAAA,GAAOtG,GAAac,IACrC,MAAMlG,EAAUkG,EAASpB,GACzB,OAAO9E,aAAmB2E,SAAW3E,EAAQuN,WAAavN,EAAQwN,OAAO,IAEvE3C,GAAS,EAAA5D,EAAA,GAAIqG,GAAepH,IACvB,CACHrF,QAAS,iBACLqF,EAASwE,KACT,oEACJ1J,KAAMiL,GAAyBwB,wBAC/BrI,WAAY,CAACc,OAGrB,OAAO2E,CACX,CAEO,SAASiB,GAAsB1G,GAClC,MAAMlB,EAAQ,GACd,IAAIwJ,GAAoB,EAAAzG,EAAA,GAAI7B,GAAauI,IAC9B,EAAA1E,EAAA,GAAO7D,GAAY,CAACrE,EAAQ6M,KAC3BD,EAAU7I,QAAQuC,SAAWuG,EAAU9I,QAAQuC,SAC9C,EAAA3D,EAAA,GAASQ,EAAO0J,IACjBA,EAAU9I,UAAYqB,GAAMC,KAG5BlC,EAAMwF,KAAKkE,GACX7M,EAAO2I,KAAKkE,IAGT7M,IACR,MAEP2M,GAAoB,EAAAG,EAAA,GAAQH,GAC5B,MAAMI,GAAoB,EAAApC,EAAA,GAAOgC,GAAoBK,GAC1CA,EAAiB3O,OAAS,IAE/ByL,GAAS,EAAA5D,EAAA,GAAI6G,GAAoBE,IACnC,MAAMC,GAAiB,EAAAhH,EAAA,GAAI+G,GAAiB9H,GACjCA,EAASwE,OAEdwD,GAAgB,OAAMF,GAAgBlJ,QAC5C,MAAO,CACHjE,QAAS,6BAA6BqN,yDACoBD,EAAeE,KAAK,WAC9EnN,KAAMiL,GAAyBmC,yBAC/BhJ,WAAY4I,EACf,IAEL,OAAOnD,CACX,CACO,SAASS,GAAqBlG,GACjC,MAAMiJ,GAAe,EAAA3C,EAAA,GAAOtG,GAAa0C,IACrC,KAAK,EAAAW,EAAA,GAAIX,EAAO,SACZ,OAAO,EAEX,MAAM2B,EAAQ3B,EAAME,MACpB,OAAOyB,IAAUtD,GAAM8B,SAAWwB,IAAUtD,GAAMC,MAAO,EAAA8B,EAAA,GAASuB,EAAM,IAEtEoB,GAAS,EAAA5D,EAAA,GAAIoH,GAAenI,IACvB,CACHrF,QAAS,iBACLqF,EAASwE,KACT,gEACJ1J,KAAMiL,GAAyBqC,yBAC/BlJ,WAAY,CAACc,OAGrB,OAAO2E,CACX,CACO,SAASU,GAAwBnG,EAAYmJ,GAChD,MAAMC,GAAe,EAAA9C,EAAA,GAAOtG,GAAa0C,QACTxF,IAApBwF,EAAMU,aAA4B,EAAA9E,EAAA,GAAS6K,EAAYzG,EAAMU,aAEnEqC,GAAS,EAAA5D,EAAA,GAAIuH,GAAe5F,IAC9B,MAAM/C,EAAM,iBAAiB+C,EAAQ8B,kEAAkE9B,EAAQJ,kCAE/G,MAAO,CACH3H,QAASgF,EACT7E,KAAMiL,GAAyBwC,yBAC/BrJ,WAAY,CAACwD,GAChB,IAEL,OAAOiC,CACX,CACO,SAASW,GAAwBpG,GACpC,MAAMyF,EAAS,GACT6D,GAAc,EAAAzF,EAAA,GAAO7D,GAAY,CAACrE,EAAQ6H,EAASQ,KACrD,MAAMpJ,EAAU4I,EAAQ9D,QACxB,OAAI9E,IAAYmG,GAAMC,MAKlB,EAAA8B,EAAA,GAASlI,GACTe,EAAO2I,KAAK,CAAEiF,IAAK3O,EAASoJ,MAAKQ,UAAWhB,KAEvC,EAAAzB,EAAA,GAASnH,IAAY4O,GAAW5O,IACrCe,EAAO2I,KAAK,CAAEiF,IAAK3O,EAAQqH,OAAQ+B,MAAKQ,UAAWhB,KAR5C7H,CAUE,GACd,IAgBH,OAfA,EAAAS,EAAA,GAAQ4D,GAAY,CAACwD,EAASiG,MAC1B,EAAArN,EAAA,GAAQkN,GAAa,EAAGC,MAAKvF,MAAKQ,gBAC9B,GAAIiF,EAAUzF,GAAO0F,GAAcH,EAAK/F,EAAQ9D,SAAU,CACtD,MAAMe,EAAM,YAAY+D,EAAUc,2EACe9B,EAAQ8B,iHAGzDG,EAAOnB,KAAK,CACR7I,QAASgF,EACT7E,KAAMiL,GAAyB8C,oBAC/B3J,WAAY,CAACwD,EAASgB,IAE9B,IACF,IAECiB,CACX,CACA,SAASiE,GAAcH,EAAK3O,GAExB,IAAI,EAAAmH,EAAA,GAASnH,GAAU,CACnB,MAAMgP,EAAchP,EAAQyH,KAAKkH,GACjC,OAAuB,OAAhBK,GAA8C,IAAtBA,EAAYC,KAC/C,CACK,IAAI,EAAAzH,EAAA,GAAWxH,GAEhB,OAAOA,EAAQ2O,EAAK,EAAG,GAAI,CAAC,GAE3B,IAAI,EAAAlG,EAAA,GAAIzI,EAAS,QAElB,OAAOA,EAAQyH,KAAKkH,EAAK,EAAG,GAAI,CAAC,GAEhC,GAAuB,kBAAZ3O,EACZ,OAAOA,IAAY2O,EAGnB,MAAMpN,MAAM,uBAEpB,CACA,SAASqN,GAAWjP,GAEhB,MAAMuP,EAAY,CACd,IACA,KACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,KAEJ,YAA0E5M,KAAlE,EAAAkB,EAAA,GAAK0L,GAAYrM,IAA0C,IAAjClD,EAAO0H,OAAOkB,QAAQ1F,IAC5D,CACO,SAAS0E,GAAgBvH,GAC5B,MAAMU,EAAQV,EAAQW,WAAa,IAAM,GAGzC,OAAO,IAAIgE,OAAO,OAAO3E,EAAQqH,UAAW3G,EAChD,CACO,SAAS4G,GAActH,GAC1B,MAAMU,EAAQV,EAAQW,WAAa,KAAO,IAG1C,OAAO,IAAIgE,OAAO,GAAG3E,EAAQqH,SAAU3G,EAC3C,CACO,SAASyO,GAAqBC,EAAiBC,EAAY1J,GAC9D,MAAMkF,EAAS,GAsDf,OApDK,EAAApC,EAAA,GAAI2G,EAAiBrK,IACtB8F,EAAOnB,KAAK,CACR7I,QAAS,sDACLkE,EACA,iCACJ/D,KAAMiL,GAAyBqD,yCAGlC,EAAA7G,EAAA,GAAI2G,EAAiBpK,IACtB6F,EAAOnB,KAAK,CACR7I,QAAS,sDACLmE,EACA,iCACJhE,KAAMiL,GAAyBsD,2CAGnC,EAAA9G,EAAA,GAAI2G,EAAiBpK,KACrB,EAAAyD,EAAA,GAAI2G,EAAiBrK,MACpB,EAAA0D,EAAA,GAAI2G,EAAgBI,MAAOJ,EAAgBK,cAC5C5E,EAAOnB,KAAK,CACR7I,QAAS,kDAAkDkE,OAAkBqK,EAAgBK,qCAE7FzO,KAAMiL,GAAyByD,sDAGnC,EAAAjH,EAAA,GAAI2G,EAAiBpK,KACrB,EAAAxD,EAAA,GAAQ4N,EAAgBI,OAAO,CAACG,EAAeC,MAC3C,EAAApO,EAAA,GAAQmO,GAAe,CAAC5F,EAAa8F,KACjC,IAAI,EAAA1H,EAAA,GAAY4B,GACZc,EAAOnB,KAAK,CACR7I,QACI,sEAAI+O,iBAA4BC,OACpC7O,KAAMiL,GAAyB6D,iDAGlC,IAAI,EAAArH,EAAA,GAAIsB,EAAa,cAAe,CACrC,MAAMV,GAAY,EAAAzF,EAAA,GAAQmG,EAAY1B,YAChC0B,EAAY1B,WACZ,CAAC0B,EAAY1B,aACnB,EAAA7G,EAAA,GAAQ6H,GAAY0G,KACX,EAAA5H,EAAA,GAAY4H,KACZ,EAAArM,EAAA,GAASiM,EAAeI,IACzBlF,EAAOnB,KAAK,CACR7I,QAAS,8DAA8DkP,EAAcrF,mBAAmBX,EAAYW,0BAA0BkF,OAC9I5O,KAAMiL,GAAyB+D,iDAEvC,GAER,IACF,IAGHnF,CACX,CACO,SAASoF,GAA4Bb,EAAiBC,EAAY1J,GACrE,MAAMuK,EAAW,GACjB,IAAIC,GAAkB,EACtB,MAAMC,GAAgB,EAAAvC,EAAA,IAAQ,EAAAwC,EAAA,IAAQ,EAAA3N,EAAA,GAAO0M,EAAgBI,SACvDc,GAAqB,EAAArK,EAAA,GAAOmK,GAAgBlK,GAAaA,EAASpB,KAAaqB,GAAMC,KACrFmK,EAAsB5H,GAAahD,GAsCzC,OArCI0J,IACA,EAAA7N,EAAA,GAAQ8O,GAAqB1H,IACzB,MAAM4H,EAAY1H,GAAsBF,EAAS2H,GACjD,IAAkB,IAAdC,EAAqB,CACrB,MAAM3P,EAAU4P,GAA2B7H,EAAS4H,GAC9CE,EAAoB,CACtB7P,UACAG,KAAMwP,EAAUG,MAChB/G,UAAWhB,GAEfsH,EAASxG,KAAKgH,EAClB,MAGQ,EAAAjI,EAAA,GAAIG,EAAS,gBACe,IAAxBA,EAAQC,cACRsH,GAAkB,GAIlB1L,EAAiB8L,EAAqB3H,EAAQ9D,WAC9CqL,GAAkB,EAG9B,IAGJd,IAAec,GACfD,EAASxG,KAAK,CACV7I,QAAS,uRAKTG,KAAMiL,GAAyB2E,uBAGhCV,CACX,CACO,SAASW,GAAiB/J,GAC7B,MAAMgK,EAAe,CAAC,EAChBC,GAAY,EAAAC,EAAA,GAAKlK,GAWvB,OAVA,EAAAtF,EAAA,GAAQuP,GAAYE,IAChB,MAAMC,EAAiBpK,EAAYmK,GAEnC,KAAI,EAAArN,EAAA,GAAQsN,GAIR,MAAM3P,MAAM,wBAHZuP,EAAaG,GAAW,EAI5B,IAEGH,CACX,CAEO,SAAS/H,GAAgBa,GAC5B,MAAM5J,EAAU4J,EAAU9E,QAE1B,IAAI,EAAAqC,EAAA,GAASnH,GACT,OAAO,EAEN,IAAI,EAAAwH,EAAA,GAAWxH,GAEhB,OAAO,EAEN,IAAI,EAAAyI,EAAA,GAAIzI,EAAS,QAElB,OAAO,EAEN,IAAI,EAAAkI,EAAA,GAASlI,GACd,OAAO,EAGP,MAAMuB,MAAM,uBAEpB,CACO,SAASyH,GAAehJ,GAC3B,UAAI,EAAAkI,EAAA,GAASlI,IAA+B,IAAnBA,EAAQZ,SACtBY,EAAQkD,WAAW,EAKlC,CAIO,MAAMiO,GAAgC,CAEzCtE,KAAM,SAAUuE,GACZ,MAAMC,EAAMD,EAAKhS,OACjB,IAAK,IAAI6B,EAAIgD,KAAKqN,UAAWrQ,EAAIoQ,EAAKpQ,IAAK,CACvC,MAAMsQ,EAAIH,EAAKlO,WAAWjC,GAC1B,GAAU,KAANsQ,EAEA,OADAtN,KAAKqN,UAAYrQ,EAAI,GACd,EAEN,GAAU,KAANsQ,EAOL,OAN+B,KAA3BH,EAAKlO,WAAWjC,EAAI,GACpBgD,KAAKqN,UAAYrQ,EAAI,EAGrBgD,KAAKqN,UAAYrQ,EAAI,GAElB,CAEf,CACA,OAAO,CACX,EACAqQ,UAAW,GAEf,SAASxI,GAAsBF,EAASF,GACpC,IAAI,EAAAD,EAAA,GAAIG,EAAS,eAGb,OAAO,EAIP,IAAI,EAAAzB,EAAA,GAASyB,EAAQ9D,SAAU,CAC3B,IAEIL,EAAiBiE,EAAyBE,EAAQ9D,QACtD,CACA,MAAOlE,GAEH,MAAO,CACH+P,MAAO1E,GAAyBuF,oBAChCC,OAAQ7Q,EAAEC,QAElB,CACA,OAAO,CACX,CACK,IAAI,EAAAqH,EAAA,GAASU,EAAQ9D,SAEtB,OAAO,EAEN,GAAIiE,GAAgBH,GAErB,MAAO,CAAE+H,MAAO1E,GAAyByF,mBAGzC,MAAMnQ,MAAM,uBAGxB,CACO,SAASkP,GAA2B7H,EAAS+I,GAEhD,GAAIA,EAAQhB,QAAU1E,GAAyBuF,oBAC3C,MACI,2FAA4B5I,EAAQ8B,oCAClBiH,EAAQF,+GAG7B,GAAIE,EAAQhB,QAAU1E,GAAyByF,kBAChD,MACI,sGAA4B9I,EAAQ8B,sHAIxC,MAAMnJ,MAAM,uBAEpB,CACA,SAASoH,GAAaiJ,GAClB,MAAMlN,GAAY,EAAAuC,EAAA,GAAI2K,GAAeC,IAC7B,EAAA3J,EAAA,GAAS2J,GACFA,EAAY3O,WAAW,GAGvB2O,IAGf,OAAOnN,CACX,CACA,SAASwF,GAAiBjD,EAAK6K,EAAKrR,QACf6B,IAAb2E,EAAI6K,GACJ7K,EAAI6K,GAAO,CAACrR,GAGZwG,EAAI6K,GAAKpI,KAAKjJ,EAEtB,CACO,MAAMqB,GAAqB,IAgBlC,IAAIiQ,GAA4B,GACzB,SAAS7P,GAAyB8H,GACrC,OAAOA,EAAWlI,GACZkI,EACA+H,GAA0B/H,EACpC,CASA,SAAShE,KACL,IAAI,EAAAyE,EAAA,GAAQsH,IAA4B,CACpCA,GAA4B,IAAIC,MAAM,OACtC,IAAK,IAAI/Q,EAAI,EAAGA,EAAI,MAAOA,IACvB8Q,GAA0B9Q,GAAKA,EAAI,IAAM,OAASA,EAAI,KAAOA,CAErE,CACJ,C,ICn2BWgL,G,iGACX,SAAWA,GACPA,EAAyBA,EAAyB,mBAAqB,GAAK,kBAC5EA,EAAyBA,EAAyB,mBAAqB,GAAK,kBAC5EA,EAAyBA,EAAyB,oBAAsB,GAAK,mBAC7EA,EAAyBA,EAAyB,2BAA6B,GAAK,0BACpFA,EAAyBA,EAAyB,4BAA8B,GAAK,2BACrFA,EAAyBA,EAAyB,4BAA8B,GAAK,2BACrFA,EAAyBA,EAAyB,4BAA8B,GAAK,2BACrFA,EAAyBA,EAAyB,yCAA2C,GAAK,wCAClGA,EAAyBA,EAAyB,2CAA6C,GAAK,0CACpGA,EAAyBA,EAAyB,sDAAwD,GAAK,qDAC/GA,EAAyBA,EAAyB,6CAA+C,IAAM,4CACvGA,EAAyBA,EAAyB,oBAAsB,IAAM,mBAC9EA,EAAyBA,EAAyB,uBAAyB,IAAM,sBACjFA,EAAyBA,EAAyB,wBAA0B,IAAM,uBAClFA,EAAyBA,EAAyB,uBAAyB,IAAM,sBACjFA,EAAyBA,EAAyB,uBAAyB,IAAM,sBACjFA,EAAyBA,EAAyB,qBAAuB,IAAM,oBAC/EA,EAAyBA,EAAyB,mDAAqD,IAAM,iDAChH,EAnBD,CAmBGA,KAA6BA,GAA2B,CAAC,IAC5D,MAAMgG,GAAuB,CACzBC,+BAA+B,EAC/BxM,iBAAkB,OAClByM,uBAAwB,YACxBxM,yBAA0B,CAAC,KAAM,MACjCtF,qBAAqB,EACrBoF,UAAU,EACV2M,qBAAsB,KACtBC,eAAe,EACfC,iBAAiB,EACjBC,iBAAiB,GAErBC,OAAOC,OAAOR,IACP,MAAM9L,GACT,WAAApC,CAAYqL,EAAiBsD,EAAST,IAoClC,GAnCAhO,KAAKmL,gBAAkBA,EACvBnL,KAAK0O,sBAAwB,GAC7B1O,KAAK2O,uBAAyB,GAC9B3O,KAAK8C,mBAAqB,CAAC,EAC3B9C,KAAK6F,6BAA+B,CAAC,EACrC7F,KAAKuL,MAAQ,GACbvL,KAAK6C,YAAc,CAAC,EACpB7C,KAAK4O,iBAAkB,EACvB5O,KAAK6O,eAAgB,EACrB7O,KAAK+C,WAAY,EACjB/C,KAAK8O,mBAAqB,CAAC,EAG3B9O,KAAK+O,WAAa,CAACC,EAAWC,KAG1B,IAA2B,IAAvBjP,KAAKoO,cAAwB,CAC7BpO,KAAKkP,kBACL,MAAMC,EAAS,IAAIpB,MAAM/N,KAAKkP,gBAAkB,GAAGhF,KAAK,MACpDlK,KAAKkP,gBAAkBlP,KAAKoP,mBAC5BC,QAAQC,IAAI,GAAGH,YAAcH,MAEjC,MAAM,KAAEO,EAAI,MAAE/S,IAAU,EAAAgT,GAAA,GAAMP,GAExBQ,EAAcF,EAAO,GAAKF,QAAQK,KAAOL,QAAQC,IAKvD,OAJItP,KAAKkP,gBAAkBlP,KAAKoP,mBAC5BK,EAAY,GAAGN,SAAcH,YAAoBO,OAErDvP,KAAKkP,kBACE1S,CACX,CAEI,OAAOyS,GACX,EAEkB,mBAAXR,EACP,MAAMnR,MAAM,8HAIhB0C,KAAKyO,QAAS,QAAO,CAAC,EAAGT,GAAsBS,GAC/C,MAAMkB,EAAe3P,KAAKyO,OAAOL,eACZ,IAAjBuB,GACA3P,KAAKoP,kBAAoBQ,IACzB5P,KAAKoO,eAAgB,GAEQ,kBAAjBuB,IACZ3P,KAAKoP,kBAAoBO,EACzB3P,KAAKoO,eAAgB,GAEzBpO,KAAKkP,iBAAmB,EACxBlP,KAAK+O,WAAW,qBAAqB,KACjC,IAAIc,EACAC,GAAoB,EACxB9P,KAAK+O,WAAW,yBAAyB,KACrC,GAAI/O,KAAKyO,OAAOP,yBACZF,GAAqBE,uBAErBlO,KAAKyO,OAAOP,uBAAyBhB,QAGrC,GAAIlN,KAAKyO,OAAO/M,2BACZsM,GAAqBtM,yBACrB,MAAMpE,MAAM,oLAIpB,GAAImR,EAAOjN,UAAYiN,EAAOrS,oBAC1B,MAAMkB,MAAM,sEAEhB0C,KAAK4O,gBAAkB,kBAAkBhG,KAAK5I,KAAKyO,OAAOhN,kBAC1DzB,KAAK6O,cAAgB,QAAQjG,KAAK5I,KAAKyO,OAAOhN,mBAE1C,EAAA9B,EAAA,GAAQwL,GACR0E,EAAmB,CACftE,MAAO,CAAEC,aAAa,EAAAuE,GAAA,GAAM5E,IAC5BK,YAAa1K,IAKjBgP,GAAoB,EACpBD,GAAmB,EAAAE,GAAA,GAAM5E,GAC7B,KAEgC,IAAhCnL,KAAKyO,OAAOJ,kBACZrO,KAAK+O,WAAW,wBAAwB,KACpC/O,KAAK0O,sBAAwB1O,KAAK0O,sBAAsB3H,OAAOmE,GAAqB2E,EAAkB7P,KAAK4O,gBAAiB5O,KAAKyO,OAAO/M,0BAA0B,IAEtK1B,KAAK+O,WAAW,+BAA+B,KAC3C/O,KAAK2O,uBAAyB3O,KAAK2O,uBAAuB5H,OAAOiF,GAA4B6D,EAAkB7P,KAAK4O,gBAAiB5O,KAAKyO,OAAO/M,0BAA0B,KAInLmO,EAAiBtE,MAAQsE,EAAiBtE,MACpCsE,EAAiBtE,MACjB,CAAC,GAGP,EAAAhO,EAAA,GAAQsS,EAAiBtE,OAAO,CAACG,EAAeC,KAC5CkE,EAAiBtE,MAAMI,IAAgB,EAAA3J,EAAA,GAAO0J,GAAgB5F,IAAgB,EAAA5B,EAAA,GAAY4B,IAAa,IAE3G,MAAMkK,GAAe,EAAAjD,EAAA,GAAK8C,EAAiBtE,OAoC3C,IAnCA,EAAAhO,EAAA,GAAQsS,EAAiBtE,OAAO,CAAC0E,EAAYC,KACzClQ,KAAK+O,WAAW,UAAUmB,iBAA2B,KAUjD,GATAlQ,KAAKuL,MAAM9F,KAAKyK,IACoB,IAAhClQ,KAAKyO,OAAOJ,iBACZrO,KAAK+O,WAAW,oBAAoB,KAChC/O,KAAK0O,sBAAwB1O,KAAK0O,sBAAsB3H,OAAOL,EAAiBuJ,EAAYD,GAAc,KAM9G,EAAAxJ,EAAA,GAAQxG,KAAK0O,uBAAwB,CAErC,IAAIyB,GADJ,SAAkBF,GAElBjQ,KAAK+O,WAAW,qBAAqB,KACjCoB,EAAoBjP,EAAkB+O,EAAY,CAC9CvO,yBAA0B1B,KAAKyO,OAAO/M,yBACtCD,iBAAkBgN,EAAOhN,iBACzBrF,oBAAqBqS,EAAOrS,oBAC5BoF,SAAUiN,EAAOjN,SACjBG,OAAQ3B,KAAK+O,YACf,IAEN/O,KAAK8C,mBAAmBoN,GACpBC,EAAkBrN,mBACtB9C,KAAK6F,6BAA6BqK,GAC9BC,EAAkBtK,6BACtB7F,KAAK6C,aAAc,QAAO,CAAC,EAAG7C,KAAK6C,YAAasN,EAAkBtN,aAClE7C,KAAK+C,UAAYoN,EAAkBpN,WAAa/C,KAAK+C,UACrD/C,KAAK8O,mBAAmBoB,GACpBC,EAAkBvK,cAC1B,IACF,IAEN5F,KAAKwL,YAAcqE,EAAiBrE,cAC/B,EAAAhF,EAAA,GAAQxG,KAAK0O,yBACb1O,KAAKyO,OAAOR,8BAA+B,CAC5C,MAAMmC,GAAiB,EAAApN,EAAA,GAAIhD,KAAK0O,uBAAwB2B,GAC7CA,EAAMzT,UAEX0T,EAAuBF,EAAelG,KAAK,6BACjD,MAAM,IAAI5M,MAAM,4CAA8CgT,EAClE,EAEA,EAAA/S,EAAA,GAAQyC,KAAK2O,wBAAyBlC,KAClC,OAAcA,EAAkB7P,QAAQ,IAE5CoD,KAAK+O,WAAW,wCAAwC,KAqBpD,GAjBI/N,GACAhB,KAAKuQ,UAAYC,GAAA,EACjBxQ,KAAKyQ,MAAQzQ,KAAK0Q,gBAGlB1Q,KAAK2Q,gBAAkBC,GAAA,EACvB5Q,KAAKyQ,MAAQzQ,KAAK6Q,eAElBf,IACA9P,KAAK8Q,YAAcF,GAAA,IAEM,IAAzB5Q,KAAK4O,kBACL5O,KAAK+Q,iBAAmBP,GAAA,IAED,IAAvBxQ,KAAK6O,gBACL7O,KAAKgR,iCAAmCJ,GAAA,GAExC,QAAQhI,KAAK5I,KAAKyO,OAAOhN,kBACzBzB,KAAKiR,oBAAsBjR,KAAKkR,qBAE/B,GAAI,aAAatI,KAAK5I,KAAKyO,OAAOhN,kBACnCzB,KAAKiR,oBAAsBjR,KAAKmR,yBAE/B,KAAI,cAAcvI,KAAK5I,KAAKyO,OAAOhN,kBAIpC,MAAMnE,MAAM,8CAA8C0C,KAAKyO,OAAOhN,qBAHtEzB,KAAKiR,oBAAsBjR,KAAKoR,qBAIpC,CACIpR,KAAK+C,WACL/C,KAAKqR,SAAWrR,KAAKsR,kBACrBtR,KAAKuR,cAAgBvR,KAAKwR,0BAG1BxR,KAAKqR,SAAWrR,KAAKyR,0BACrBzR,KAAKuR,cAAgBvR,KAAK0R,sBAC9B,IAEJ1R,KAAK+O,WAAW,gCAAgC,KAC5C,MAAM4C,GAAmB,EAAA3M,EAAA,GAAOhF,KAAK8O,oBAAoB,CAAC8C,EAAmBhM,EAAgBiM,MAClE,IAAnBjM,GACAgM,EAAkBnM,KAAKoM,GAEpBD,IACR,IACH,GAAInD,EAAOrS,uBAAwB,EAAAoK,EAAA,GAAQmL,GACvC,MAAMrU,MAAM,kBAAkBqU,EAAiBzH,KAAK,qOAGxD,IAEJlK,KAAK+O,WAAW,0BAA0B,KACtC/S,GAAwB,IAE5BgE,KAAK+O,WAAW,oBAAoB,MAChC,QAAiB/O,KAAK,GACxB,GAEV,CACA,QAAA8R,CAAS3E,EAAM4E,EAAc/R,KAAKwL,aAC9B,KAAK,EAAAhF,EAAA,GAAQxG,KAAK0O,uBAAwB,CACtC,MAAM0B,GAAiB,EAAApN,EAAA,GAAIhD,KAAK0O,uBAAwB2B,GAC7CA,EAAMzT,UAEX0T,EAAuBF,EAAelG,KAAK,6BACjD,MAAM,IAAI5M,MAAM,uEACZgT,EACR,CACA,OAAOtQ,KAAKgS,iBAAiB7E,EAAM4E,EACvC,CAKA,gBAAAC,CAAiB7E,EAAM4E,GACnB,IAAI/U,EAAGiV,EAAGC,EAAGC,EAAe/M,EAAWgN,EAAcC,EAASC,EAAYC,EAAa/M,EAAOb,EAAS6N,EAAUC,EAAwB7Q,EAAK6O,EAC9I,MAAMiC,EAAUvF,EACVwF,EAAYD,EAAQvX,OAC1B,IAAIyX,EAAS,EACTC,EAAqB,EAKzB,MAAMC,EAAwB9S,KAAK+C,UAC7B,EACAgQ,KAAKC,MAAM7F,EAAKhS,OAAS,IACzB8X,EAAgB,IAAIlF,MAAM+E,GAC1BlM,EAAS,GACf,IAAIxL,EAAO4E,KAAK4O,gBAAkB,OAAIvQ,EAClChD,EAAS2E,KAAK4O,gBAAkB,OAAIvQ,EACxC,MAAM6U,EAAStG,GAAiB5M,KAAK6C,aAC/BuI,EAAapL,KAAK4O,gBAClBuE,EAAwBnT,KAAKyO,OAAOP,uBAC1C,IAAIkF,EAAyB,EACzBtQ,EAAqB,GACrBuQ,EAAmC,GACvC,MAAMC,EAAY,GACZC,EAAa,GAEnB,IAAIC,EACJ,SAASC,IACL,OAAO3Q,CACX,CACA,SAAS4Q,EAA6B3N,GAClC,MAAMrH,EAAmBT,GAAyB8H,GAC5C4N,EAAmBN,EAAiC3U,GAC1D,YAAyBL,IAArBsV,EACOJ,EAGAI,CAEf,CAdApF,OAAOC,OAAO+E,GAed,MAAMK,EAAYC,IAEd,GAAyB,IAArBP,EAAUnY,aAGuBkD,IAAjCwV,EAASlO,UAAUpB,UAAyB,CAG5C,MAAM3C,EAAM5B,KAAKyO,OAAON,qBAAqBtT,iCAAiCgZ,GAC9EjN,EAAOnB,KAAK,CACRmN,OAAQiB,EAAS3Y,YACjBE,KAAMyY,EAASC,UACfzY,OAAQwY,EAASE,YACjB5Y,OAAQ0Y,EAAS9Y,MAAMI,OACvByB,QAASgF,GAEjB,KACK,CACD0R,EAAU5N,MACV,MAAMsO,GAAU,EAAAC,GAAA,GAAKX,GACrBxQ,EAAqB9C,KAAK8C,mBAAmBkR,GAC7CX,EACIrT,KAAK6F,6BAA6BmO,GACtCZ,EAAyBtQ,EAAmB3H,OAC5C,MAAM+Y,EAAqBlU,KAAK8O,mBAAmBkF,KAAqC,IAAzBhU,KAAKyO,OAAOjN,SAEvEgS,EADAH,GAAoCa,EACdR,EAGAD,CAE9B,GAEJ,SAASU,EAAUH,GACfV,EAAU7N,KAAKuO,GACfX,EACIrT,KAAK6F,6BAA6BmO,GACtClR,EAAqB9C,KAAK8C,mBAAmBkR,GAC7CZ,EAAyBtQ,EAAmB3H,OAC5CiY,EAAyBtQ,EAAmB3H,OAC5C,MAAM+Y,EAAqBlU,KAAK8O,mBAAmBkF,KAAqC,IAAzBhU,KAAKyO,OAAOjN,SAEvEgS,EADAH,GAAoCa,EACdR,EAGAD,CAE9B,CAIA,IAAIW,EADJD,EAAUE,KAAKrU,KAAM+R,GAErB,MAAMzD,EAAkBtO,KAAKyO,OAAOH,gBACpC,MAAOsE,EAASD,EAAW,CACvBP,EAAe,KACf,MAAMkC,EAAe5B,EAAQzT,WAAW2T,GAClC2B,EAA2Bf,EAAoBc,GAC/CE,EAAuBD,EAAyBpZ,OACtD,IAAK6B,EAAI,EAAGA,EAAIwX,EAAsBxX,IAAK,CACvCoX,EAAaG,EAAyBvX,GACtC,MAAMiG,EAAcmR,EAAWrY,QAC/BsW,EAAU,KAEV,MAAMoC,EAAiBL,EAAW7O,MAuBlC,IAtBuB,IAAnBkP,EACIH,IAAiBG,IAEjBrC,EAAenP,IAGU,IAAxBmR,EAAW9O,UAChBmL,EAAQxN,EAAYO,KAAKkP,EAASE,EAAQK,EAAeC,GAC3C,OAAVzC,GACA2B,EAAe3B,EAAM,QACCpS,IAAlBoS,EAAM4B,UACNA,EAAU5B,EAAM4B,UAIpBD,EAAe,OAInBpS,KAAK2Q,gBAAgB1N,EAAa2P,GAClCR,EAAepS,KAAKyQ,MAAMxN,EAAakK,EAAMyF,IAE5B,OAAjBR,EAAuB,CAIvB,GADAhN,EAAYgP,EAAWhP,eACL/G,IAAd+G,EAAyB,CAGzB,MAAMsP,EAAkBtP,EAAUjK,OAClC,IAAK+W,EAAI,EAAGA,EAAIwC,EAAiBxC,IAAK,CAClC,MAAMyC,EAAkB7R,EAAmBsC,EAAU8M,IAC/C0C,EAAmBD,EAAgB5Y,QAoBzC,GAnBAuW,EAAa,MAGoB,IAA7BqC,EAAgBrP,UAChBmL,EAAQmE,EAAiBpR,KAAKkP,EAASE,EAAQK,EAAeC,GAChD,OAAVzC,GACA0B,EAAgB1B,EAAM,QACApS,IAAlBoS,EAAM4B,UACNC,EAAa7B,EAAM4B,UAIvBF,EAAgB,OAIpBnS,KAAK2Q,gBAAgBiE,EAAkBhC,GACvCT,EAAgBnS,KAAKyQ,MAAMmE,EAAkBzH,EAAMyF,IAEnDT,GAAiBA,EAAchX,OAASiX,EAAajX,OAAQ,CAC7DiX,EAAeD,EACfE,EAAUC,EACV8B,EAAaO,EAGb,KACJ,CACJ,CACJ,CACA,KACJ,CACJ,CAEA,GAAqB,OAAjBvC,EAAuB,CAqBvB,GApBAG,EAAcH,EAAajX,OAC3BqK,EAAQ4O,EAAW5O,WACLnH,IAAVmH,IACAb,EAAUyP,EAAWxQ,aAGrB4O,EAAWxS,KAAKiR,oBAAoBmB,EAAcQ,EAAQjO,EAASyP,EAAWzO,UAAWvK,EAAMC,EAAQkX,GACvGvS,KAAKuR,cAAciB,EAAUH,IAEf,IAAV7M,EACAqN,EAAqB7S,KAAKqR,SAAS4B,EAAeJ,EAAoBL,GAGtEU,EAAO1N,GAAOC,KAAK+M,IAG3BrF,EAAOnN,KAAKuQ,UAAUpD,EAAMoF,GAC5BK,GAAkBL,EAElBlX,EAAS2E,KAAK+Q,iBAAiB1V,EAAQkX,IACpB,IAAfnH,IAAwD,IAAjCgJ,EAAW/O,kBAA4B,CAC9D,IACIwP,EACAC,EAFAC,EAAkB,EAGtB5B,EAAsB9F,UAAY,EAClC,GACIwH,EAAkB1B,EAAsBvK,KAAKwJ,IACrB,IAApByC,IACAC,EAAkB3B,EAAsB9F,UAAY,EACpD0H,YAEqB,IAApBF,GACe,IAApBE,IACA3Z,GAAc2Z,EACd1Z,EAASkX,EAAcuC,EACvB9U,KAAKgR,iCAAiCwB,EAAUhN,EAAOsP,EAAiBC,EAAiB3Z,EAAMC,EAAQkX,GAE/G,CAEAvS,KAAK8Q,YAAYsD,EAAYR,EAAUO,EAAW3B,EACtD,KACK,CAED,MAAMwC,EAAmBpC,EACnBqC,EAAY7Z,EACZ8Z,EAAc7Z,EACpB,IAAI8Z,GAAuC,IAApB7G,EACvB,OAA4B,IAArB6G,GAA8BvC,EAASD,EAI1C,IAFAxF,EAAOnN,KAAKuQ,UAAUpD,EAAM,GAC5ByF,IACKX,EAAI,EAAGA,EAAImB,EAAwBnB,IAAK,CACzC,MAAMmC,EAAatR,EAAmBmP,GAChChP,EAAcmR,EAAWrY,QAEzB0Y,EAAiBL,EAAW7O,MAelC,IAduB,IAAnBkP,EACI/B,EAAQzT,WAAW2T,KAAY6B,IAE/BU,GAAmB,IAGM,IAAxBf,EAAW9O,SAChB6P,EACiE,OAA7DlS,EAAYO,KAAKkP,EAASE,EAAQK,EAAeC,IAGrDlT,KAAK2Q,gBAAgB1N,EAAa2P,GAClCuC,EAA8C,OAA3BlS,EAAYO,KAAK2J,KAEf,IAArBgI,EACA,KAER,CAaJ,GAXA1C,EAAYG,EAASoC,EACrB3Z,EAAS2E,KAAK+Q,iBAAiB1V,EAAQoX,GAEvC7Q,EAAM5B,KAAKyO,OAAON,qBAAqBnT,iCAAiC0X,EAASsC,EAAkBvC,EAAWwC,EAAWC,GACzHtO,EAAOnB,KAAK,CACRmN,OAAQoC,EACR5Z,KAAM6Z,EACN5Z,OAAQ6Z,EACR/Z,OAAQsX,EACR7V,QAASgF,KAEW,IAApB0M,EACA,KAER,CACJ,CAOA,OAJKtO,KAAK+C,YAENkQ,EAAc9X,OAAS0X,GAEpB,CACHuC,OAAQnC,EACRC,OAAQA,EACRtM,OAAQA,EAEhB,CACA,WAAAkK,CAAYrC,EAAQmF,EAAUO,EAAW3B,GACrC,IAAmB,IAAf/D,EAAO/I,IAAc,CAGrB,MAAM2P,EAAW5G,EAAOhJ,KACxBmO,EAASpB,QACQnU,IAAbgX,GACAlB,EAAUE,KAAKrU,KAAMqV,EAE7B,WACyBhX,IAAhBoQ,EAAOhJ,MACZ0O,EAAUE,KAAKrU,KAAMyO,EAAOhJ,KAEpC,CACA,SAAA8K,CAAUpD,EAAMhS,GACZ,OAAOgS,EAAKmI,UAAUna,EAC1B,CACA,eAAAwV,CAAgBjV,EAAQ6Z,GACpB7Z,EAAO2R,UAAYkI,CACvB,CAEA,gCAAAvE,CAAiCwB,EAAUhN,EAAOgQ,EAAWT,EAAiB3Z,EAAMC,EAAQkX,GACxF,IAAIkD,EAAcC,OACJrX,IAAVmH,IAEAiQ,EAAeD,IAAcjD,EAAc,EAC3CmD,EAAmBD,GAAgB,EAAI,EACb,IAApBV,IAA0C,IAAjBU,IAE3BjD,EAASmD,QAAUva,EAAOsa,EAG1BlD,EAASoD,UAAYva,EAAS,EAAKqa,GAI/C,CACA,gBAAA3E,CAAiB8E,EAAWtD,GACxB,OAAOsD,EAAYtD,CACvB,CACA,qBAAAnB,CAAsBrW,EAAOG,EAAa0I,EAAc+B,GACpD,MAAO,CACH5K,QACAG,cACA0I,eACA+B,YAER,CACA,oBAAAwL,CAAqBpW,EAAOG,EAAa0I,EAAc+B,EAAWmO,EAAWC,GACzE,MAAO,CACHhZ,QACAG,cACA4Y,YACAC,cACAnQ,eACA+B,YAER,CACA,eAAAuL,CAAgBnW,EAAOG,EAAa0I,EAAc+B,EAAWmO,EAAWC,EAAaxB,GACjF,MAAO,CACHxX,QACAG,cACA4a,UAAW5a,EAAcqX,EAAc,EACvCuB,YACA6B,QAAS7B,EACTC,cACA6B,UAAW7B,EAAcxB,EAAc,EACvC3O,eACA+B,YAER,CACA,iBAAA2L,CAAkByE,EAAa/K,EAAOgL,GAElC,OADAD,EAAYtQ,KAAKuQ,GACVhL,CACX,CACA,yBAAAyG,CAA0BsE,EAAa/K,EAAOgL,GAG1C,OAFAD,EAAY/K,GAASgL,EACrBhL,IACOA,CACX,CACA,qBAAA0G,CAAsB5W,EAAOuX,GAAW,CACxC,uBAAAb,CAAwB1W,EAAOuX,GACX,OAAZA,IACAvX,EAAMuX,QAAUA,EAExB,CACA,aAAA3B,CAAc3U,EAASoR,EAAMyF,GACzB,MAAM3S,EAAQlE,EAAQ6M,KAAKuE,GAC3B,OAAc,IAAVlN,EACOkN,EAAKmI,UAAU1C,EAAQ7W,EAAQsR,WAEnC,IACX,CACA,aAAAwD,CAAc9U,EAASoR,GACnB,MAAMpC,EAAchP,EAAQyH,KAAK2J,GACjC,OAAuB,OAAhBpC,EAAuBA,EAAY,GAAK,IACnD,EAEJ7I,GAAM8B,QAAU,6LAEhB9B,GAAMC,GAAK,gB,gQCnoBJ,SAAS8T,EAAuBC,EAAaC,GAChD,MAAMC,EAAeF,EAAYtS,aACjC,OAAIwS,IAAiBD,EAAevS,eAII,IAA5BuS,EAAeE,WACiC,IAApDF,EAAeG,mBAAmBF,EAE9C,CAGO,SAASG,EAAmCzb,EAAO6J,GACtD,OAAO7J,EAAM8I,eAAiBe,EAAQf,YAC1C,CACO,IAAI4S,EAAoB,EACxB,MAAMC,EAAkB,CAAC,EACzB,SAASC,EAAkBvV,GAE9B,MAAMwV,EAAuBC,EAAiBzV,GAE9C0V,EAAwBF,GAExBG,EAAwBH,GACxBI,EAA2BJ,IAC3B,OAAQA,GAAuBhS,IAC3BA,EAAQ0R,SAAW1R,EAAQqS,gBAAgB7b,OAAS,CAAC,GAE7D,CACO,SAASyb,EAAiBzV,GAC7B,IAAIrE,GAAS,OAAMqE,GACf8V,EAAa9V,EACb+V,GAAY,EAChB,MAAOA,EAAW,CACdD,GAAa,QAAQ,QAAQ,OAAIA,GAAanR,GAAgBA,EAAYqR,eAC1E,MAAMC,GAAgB,OAAWH,EAAYna,GAC7CA,EAASA,EAAOiK,OAAOqQ,IACnB,OAAQA,GACRF,GAAY,EAGZD,EAAaG,CAErB,CACA,OAAOta,CACX,CACO,SAAS+Z,EAAwB1V,IACpC,OAAQA,GAAa2E,IACZuR,EAAoBvR,KACrB2Q,EAAgBD,GAAqB1Q,EACrCA,EAAYlC,aAAe4S,KAG3Bc,EAAsBxR,MACrB,OAAQA,EAAYqR,cAIrBrR,EAAYqR,WAAa,CAACrR,EAAYqR,aAErCG,EAAsBxR,KACvBA,EAAYqR,WAAa,IAExBI,EAAgCzR,KACjCA,EAAYkR,gBAAkB,IAE7BQ,EAAmC1R,KACpCA,EAAYwQ,mBAAqB,CAAC,EACtC,GAER,CACO,SAASS,EAA2B5V,IACvC,OAAQA,GAAa2E,IAEjBA,EAAYkR,gBAAkB,IAC9B,OAAQlR,EAAYwQ,oBAAoB,CAACmB,EAAK5J,KAC1C/H,EAAYkR,gBAAgBvR,KAAKgR,EAAgB5I,GAAKjK,aAAa,GACrE,GAEV,CACO,SAASkT,EAAwB3V,IACpC,OAAQA,GAAa2E,IACjB4R,EAA8B,GAAI5R,EAAY,GAEtD,CACO,SAAS4R,EAA8BC,EAAMC,IAChD,OAAQD,GAAOE,IACXD,EAAStB,mBAAmBuB,EAASjU,eAAgB,CAAI,KAE7D,OAAQgU,EAAST,YAAaW,IAC1B,MAAMC,EAAUJ,EAAK5Q,OAAO6Q,IAEvB,OAASG,EAASD,IACnBJ,EAA8BK,EAASD,EAC3C,GAER,CACO,SAAST,EAAoB1S,GAChC,OAAO,OAAIA,EAAS,eACxB,CACO,SAAS2S,EAAsB3S,GAClC,OAAO,OAAIA,EAAS,aACxB,CACO,SAAS4S,EAAgC5S,GAC5C,OAAO,OAAIA,EAAS,kBACxB,CACO,SAAS6S,EAAmC7S,GAC/C,OAAO,OAAIA,EAAS,qBACxB,CACO,SAASqT,EAAYrT,GACxB,OAAO,OAAIA,EAAS,eACxB,C,oOC7GO,SAASsT,EAAWtT,GACvB,OAAIuT,EAAcvT,GACPA,EAAQwT,MAGRxT,EAAQ8B,IAEvB,CAIO,SAASyR,EAAcE,GAC1B,OAAO,OAASA,EAAID,QAAwB,KAAdC,EAAID,KACtC,CACA,MAAME,EAAS,SACTlB,EAAa,aACbgB,EAAQ,QACRpU,EAAQ,QACRQ,EAAY,YACZ+T,EAAW,WACXlU,EAAa,aACbQ,EAAc,cACdsB,EAAmB,mBAClB,SAASqS,EAAY9J,GACxB,OAAO+J,EAAoB/J,EAC/B,CACA,SAAS+J,EAAoB/J,GACzB,MAAM1S,EAAU0S,EAAO1S,QACjB4J,EAAY,CAAC,EAKnB,GAJAA,EAAUc,KAAOgI,EAAOhI,MACnB,OAAY1K,KACb4J,EAAU9E,QAAU9E,IAEpB,OAAI0S,EAAQ4J,GACZ,KAAM,4IA6BV,OA1BI,OAAI5J,EAAQ0I,KAEZxR,EAAUwR,WAAa1I,EAAO0I,KAElC,QAAkB,CAACxR,KACf,OAAI8I,EAAQ0J,KACZxS,EAAUwS,MAAQ1J,EAAO0J,KAEzB,OAAI1J,EAAQ1K,KACZ4B,EAAU5B,MAAQ0K,EAAO1K,KAEzB,OAAI0K,EAAQ6J,KACZ3S,EAAU2S,SAAW7J,EAAO6J,KAE5B,OAAI7J,EAAQlK,KACZoB,EAAUpB,UAAYkK,EAAOlK,KAE7B,OAAIkK,EAAQrK,KACZuB,EAAUvB,WAAaqK,EAAOrK,KAE9B,OAAIqK,EAAQ7J,KACZe,EAAUf,YAAc6J,EAAO7J,KAE/B,OAAI6J,EAAQvI,KACZP,EAAUO,iBAAmBuI,EAAOvI,IAEjCP,CACX,CACO,MAAM8S,EAAMF,EAAY,CAAE9R,KAAM,MAAO1K,QAAS,IAAMoG,KAEtD,SAAS8O,EAAoBtM,EAAS5J,EAAOG,EAAa4a,EAAWhC,EAAW6B,EAAS5B,EAAa6B,GACzG,MAAO,CACH7a,QACAG,cACA4a,YACAhC,YACA6B,UACA5B,cACA6B,YACAhS,aAAce,EAAQf,aACtB+B,UAAWhB,EAEnB,CACO,SAAS+T,EAAa5d,EAAO6J,GAChC,OAAO,QAAuB7J,EAAO6J,EACzC,EAhBA,QAAkB,CAAC8T,G","sources":["webpack://my-project/./node_modules/chevrotain/lib/src/scan/lexer_errors_public.js","webpack://my-project/./node_modules/chevrotain/lib/src/scan/reg_exp_parser.js","webpack://my-project/./node_modules/chevrotain/lib/src/scan/reg_exp.js","webpack://my-project/./node_modules/chevrotain/lib/src/scan/lexer.js","webpack://my-project/./node_modules/chevrotain/lib/src/scan/lexer_public.js","webpack://my-project/./node_modules/chevrotain/lib/src/scan/tokens.js","webpack://my-project/./node_modules/chevrotain/lib/src/scan/tokens_public.js"],"sourcesContent":["export const defaultLexerErrorProvider = {\n    buildUnableToPopLexerModeMessage(token) {\n        return `Unable to pop Lexer Mode after encountering Token ->${token.image}<- The Mode Stack is empty`;\n    },\n    buildUnexpectedCharactersMessage(fullText, startOffset, length, line, column) {\n        return (`unexpected character: ->${fullText.charAt(startOffset)}<- at offset: ${startOffset},` + ` skipped ${length} characters.`);\n    },\n};\n//# sourceMappingURL=lexer_errors_public.js.map","import { RegExpParser, } from \"@chevrotain/regexp-to-ast\";\nlet regExpAstCache = {};\nconst regExpParser = new RegExpParser();\nexport function getRegExpAst(regExp) {\n    const regExpStr = regExp.toString();\n    if (regExpAstCache.hasOwnProperty(regExpStr)) {\n        return regExpAstCache[regExpStr];\n    }\n    else {\n        const regExpAst = regExpParser.pattern(regExpStr);\n        regExpAstCache[regExpStr] = regExpAst;\n        return regExpAst;\n    }\n}\nexport function clearRegExpParserCache() {\n    regExpAstCache = {};\n}\n//# sourceMappingURL=reg_exp_parser.js.map","import { BaseRegExpVisitor, } from \"@chevrotain/regexp-to-ast\";\nimport { every, find, forEach, includes, isArray, values } from \"lodash-es\";\nimport { PRINT_ERROR, PRINT_WARNING } from \"@chevrotain/utils\";\nimport { getRegExpAst } from \"./reg_exp_parser.js\";\nimport { charCodeToOptimizedIndex, minOptimizationVal } from \"./lexer.js\";\nconst complementErrorMessage = \"Complement Sets are not supported for first char optimization\";\nexport const failedOptimizationPrefixMsg = 'Unable to use \"first char\" lexer optimizations:\\n';\nexport function getOptimizedStartCodesIndices(regExp, ensureOptimizations = false) {\n    try {\n        const ast = getRegExpAst(regExp);\n        const firstChars = firstCharOptimizedIndices(ast.value, {}, ast.flags.ignoreCase);\n        return firstChars;\n    }\n    catch (e) {\n        /* istanbul ignore next */\n        // Testing this relies on the regexp-to-ast library having a bug... */\n        // TODO: only the else branch needs to be ignored, try to fix with newer prettier / tsc\n        if (e.message === complementErrorMessage) {\n            if (ensureOptimizations) {\n                PRINT_WARNING(`${failedOptimizationPrefixMsg}` +\n                    `\\tUnable to optimize: < ${regExp.toString()} >\\n` +\n                    \"\\tComplement Sets cannot be automatically optimized.\\n\" +\n                    \"\\tThis will disable the lexer's first char optimizations.\\n\" +\n                    \"\\tSee: https://chevrotain.io/docs/guide/resolving_lexer_errors.html#COMPLEMENT for details.\");\n            }\n        }\n        else {\n            let msgSuffix = \"\";\n            if (ensureOptimizations) {\n                msgSuffix =\n                    \"\\n\\tThis will disable the lexer's first char optimizations.\\n\" +\n                        \"\\tSee: https://chevrotain.io/docs/guide/resolving_lexer_errors.html#REGEXP_PARSING for details.\";\n            }\n            PRINT_ERROR(`${failedOptimizationPrefixMsg}\\n` +\n                `\\tFailed parsing: < ${regExp.toString()} >\\n` +\n                `\\tUsing the @chevrotain/regexp-to-ast library\\n` +\n                \"\\tPlease open an issue at: https://github.com/chevrotain/chevrotain/issues\" +\n                msgSuffix);\n        }\n    }\n    return [];\n}\nexport function firstCharOptimizedIndices(ast, result, ignoreCase) {\n    switch (ast.type) {\n        case \"Disjunction\":\n            for (let i = 0; i < ast.value.length; i++) {\n                firstCharOptimizedIndices(ast.value[i], result, ignoreCase);\n            }\n            break;\n        case \"Alternative\":\n            const terms = ast.value;\n            for (let i = 0; i < terms.length; i++) {\n                const term = terms[i];\n                // skip terms that cannot effect the first char results\n                switch (term.type) {\n                    case \"EndAnchor\":\n                    // A group back reference cannot affect potential starting char.\n                    // because if a back reference is the first production than automatically\n                    // the group being referenced has had to come BEFORE so its codes have already been added\n                    case \"GroupBackReference\":\n                    // assertions do not affect potential starting codes\n                    case \"Lookahead\":\n                    case \"NegativeLookahead\":\n                    case \"StartAnchor\":\n                    case \"WordBoundary\":\n                    case \"NonWordBoundary\":\n                        continue;\n                }\n                const atom = term;\n                switch (atom.type) {\n                    case \"Character\":\n                        addOptimizedIdxToResult(atom.value, result, ignoreCase);\n                        break;\n                    case \"Set\":\n                        if (atom.complement === true) {\n                            throw Error(complementErrorMessage);\n                        }\n                        forEach(atom.value, (code) => {\n                            if (typeof code === \"number\") {\n                                addOptimizedIdxToResult(code, result, ignoreCase);\n                            }\n                            else {\n                                // range\n                                const range = code;\n                                // cannot optimize when ignoreCase is\n                                if (ignoreCase === true) {\n                                    for (let rangeCode = range.from; rangeCode <= range.to; rangeCode++) {\n                                        addOptimizedIdxToResult(rangeCode, result, ignoreCase);\n                                    }\n                                }\n                                // Optimization (2 orders of magnitude less work for very large ranges)\n                                else {\n                                    // handle unoptimized values\n                                    for (let rangeCode = range.from; rangeCode <= range.to && rangeCode < minOptimizationVal; rangeCode++) {\n                                        addOptimizedIdxToResult(rangeCode, result, ignoreCase);\n                                    }\n                                    // Less common charCode where we optimize for faster init time, by using larger \"buckets\"\n                                    if (range.to >= minOptimizationVal) {\n                                        const minUnOptVal = range.from >= minOptimizationVal\n                                            ? range.from\n                                            : minOptimizationVal;\n                                        const maxUnOptVal = range.to;\n                                        const minOptIdx = charCodeToOptimizedIndex(minUnOptVal);\n                                        const maxOptIdx = charCodeToOptimizedIndex(maxUnOptVal);\n                                        for (let currOptIdx = minOptIdx; currOptIdx <= maxOptIdx; currOptIdx++) {\n                                            result[currOptIdx] = currOptIdx;\n                                        }\n                                    }\n                                }\n                            }\n                        });\n                        break;\n                    case \"Group\":\n                        firstCharOptimizedIndices(atom.value, result, ignoreCase);\n                        break;\n                    /* istanbul ignore next */\n                    default:\n                        throw Error(\"Non Exhaustive Match\");\n                }\n                // reached a mandatory production, no more **start** codes can be found on this alternative\n                const isOptionalQuantifier = atom.quantifier !== undefined && atom.quantifier.atLeast === 0;\n                if (\n                // A group may be optional due to empty contents /(?:)/\n                // or if everything inside it is optional /((a)?)/\n                (atom.type === \"Group\" && isWholeOptional(atom) === false) ||\n                    // If this term is not a group it may only be optional if it has an optional quantifier\n                    (atom.type !== \"Group\" && isOptionalQuantifier === false)) {\n                    break;\n                }\n            }\n            break;\n        /* istanbul ignore next */\n        default:\n            throw Error(\"non exhaustive match!\");\n    }\n    // console.log(Object.keys(result).length)\n    return values(result);\n}\nfunction addOptimizedIdxToResult(code, result, ignoreCase) {\n    const optimizedCharIdx = charCodeToOptimizedIndex(code);\n    result[optimizedCharIdx] = optimizedCharIdx;\n    if (ignoreCase === true) {\n        handleIgnoreCase(code, result);\n    }\n}\nfunction handleIgnoreCase(code, result) {\n    const char = String.fromCharCode(code);\n    const upperChar = char.toUpperCase();\n    /* istanbul ignore else */\n    if (upperChar !== char) {\n        const optimizedCharIdx = charCodeToOptimizedIndex(upperChar.charCodeAt(0));\n        result[optimizedCharIdx] = optimizedCharIdx;\n    }\n    else {\n        const lowerChar = char.toLowerCase();\n        if (lowerChar !== char) {\n            const optimizedCharIdx = charCodeToOptimizedIndex(lowerChar.charCodeAt(0));\n            result[optimizedCharIdx] = optimizedCharIdx;\n        }\n    }\n}\nfunction findCode(setNode, targetCharCodes) {\n    return find(setNode.value, (codeOrRange) => {\n        if (typeof codeOrRange === \"number\") {\n            return includes(targetCharCodes, codeOrRange);\n        }\n        else {\n            // range\n            const range = codeOrRange;\n            return (find(targetCharCodes, (targetCode) => range.from <= targetCode && targetCode <= range.to) !== undefined);\n        }\n    });\n}\nfunction isWholeOptional(ast) {\n    const quantifier = ast.quantifier;\n    if (quantifier && quantifier.atLeast === 0) {\n        return true;\n    }\n    if (!ast.value) {\n        return false;\n    }\n    return isArray(ast.value)\n        ? every(ast.value, isWholeOptional)\n        : isWholeOptional(ast.value);\n}\nclass CharCodeFinder extends BaseRegExpVisitor {\n    constructor(targetCharCodes) {\n        super();\n        this.targetCharCodes = targetCharCodes;\n        this.found = false;\n    }\n    visitChildren(node) {\n        // No need to keep looking...\n        if (this.found === true) {\n            return;\n        }\n        // switch lookaheads as they do not actually consume any characters thus\n        // finding a charCode at lookahead context does not mean that regexp can actually contain it in a match.\n        switch (node.type) {\n            case \"Lookahead\":\n                this.visitLookahead(node);\n                return;\n            case \"NegativeLookahead\":\n                this.visitNegativeLookahead(node);\n                return;\n        }\n        super.visitChildren(node);\n    }\n    visitCharacter(node) {\n        if (includes(this.targetCharCodes, node.value)) {\n            this.found = true;\n        }\n    }\n    visitSet(node) {\n        if (node.complement) {\n            if (findCode(node, this.targetCharCodes) === undefined) {\n                this.found = true;\n            }\n        }\n        else {\n            if (findCode(node, this.targetCharCodes) !== undefined) {\n                this.found = true;\n            }\n        }\n    }\n}\nexport function canMatchCharCode(charCodes, pattern) {\n    if (pattern instanceof RegExp) {\n        const ast = getRegExpAst(pattern);\n        const charCodeFinder = new CharCodeFinder(charCodes);\n        charCodeFinder.visit(ast);\n        return charCodeFinder.found;\n    }\n    else {\n        return (find(pattern, (char) => {\n            return includes(charCodes, char.charCodeAt(0));\n        }) !== undefined);\n    }\n}\n//# sourceMappingURL=reg_exp.js.map","import { BaseRegExpVisitor } from \"@chevrotain/regexp-to-ast\";\nimport { Lexer, LexerDefinitionErrorType, } from \"./lexer_public.js\";\nimport { compact, defaults, difference, filter, find, first, flatten, forEach, has, includes, indexOf, isArray, isEmpty, isFunction, isRegExp, isString, isUndefined, keys, map, reduce, reject, values, } from \"lodash-es\";\nimport { PRINT_ERROR } from \"@chevrotain/utils\";\nimport { canMatchCharCode, failedOptimizationPrefixMsg, getOptimizedStartCodesIndices, } from \"./reg_exp.js\";\nimport { getRegExpAst } from \"./reg_exp_parser.js\";\nconst PATTERN = \"PATTERN\";\nexport const DEFAULT_MODE = \"defaultMode\";\nexport const MODES = \"modes\";\nexport let SUPPORT_STICKY = typeof new RegExp(\"(?:)\").sticky === \"boolean\";\nexport function disableSticky() {\n    SUPPORT_STICKY = false;\n}\nexport function enableSticky() {\n    SUPPORT_STICKY = true;\n}\nexport function analyzeTokenTypes(tokenTypes, options) {\n    options = defaults(options, {\n        useSticky: SUPPORT_STICKY,\n        debug: false,\n        safeMode: false,\n        positionTracking: \"full\",\n        lineTerminatorCharacters: [\"\\r\", \"\\n\"],\n        tracer: (msg, action) => action(),\n    });\n    const tracer = options.tracer;\n    tracer(\"initCharCodeToOptimizedIndexMap\", () => {\n        initCharCodeToOptimizedIndexMap();\n    });\n    let onlyRelevantTypes;\n    tracer(\"Reject Lexer.NA\", () => {\n        onlyRelevantTypes = reject(tokenTypes, (currType) => {\n            return currType[PATTERN] === Lexer.NA;\n        });\n    });\n    let hasCustom = false;\n    let allTransformedPatterns;\n    tracer(\"Transform Patterns\", () => {\n        hasCustom = false;\n        allTransformedPatterns = map(onlyRelevantTypes, (currType) => {\n            const currPattern = currType[PATTERN];\n            /* istanbul ignore else */\n            if (isRegExp(currPattern)) {\n                const regExpSource = currPattern.source;\n                if (regExpSource.length === 1 &&\n                    // only these regExp meta characters which can appear in a length one regExp\n                    regExpSource !== \"^\" &&\n                    regExpSource !== \"$\" &&\n                    regExpSource !== \".\" &&\n                    !currPattern.ignoreCase) {\n                    return regExpSource;\n                }\n                else if (regExpSource.length === 2 &&\n                    regExpSource[0] === \"\\\\\" &&\n                    // not a meta character\n                    !includes([\n                        \"d\",\n                        \"D\",\n                        \"s\",\n                        \"S\",\n                        \"t\",\n                        \"r\",\n                        \"n\",\n                        \"t\",\n                        \"0\",\n                        \"c\",\n                        \"b\",\n                        \"B\",\n                        \"f\",\n                        \"v\",\n                        \"w\",\n                        \"W\",\n                    ], regExpSource[1])) {\n                    // escaped meta Characters: /\\+/ /\\[/\n                    // or redundant escaping: /\\a/\n                    // without the escaping \"\\\"\n                    return regExpSource[1];\n                }\n                else {\n                    return options.useSticky\n                        ? addStickyFlag(currPattern)\n                        : addStartOfInput(currPattern);\n                }\n            }\n            else if (isFunction(currPattern)) {\n                hasCustom = true;\n                // CustomPatternMatcherFunc - custom patterns do not require any transformations, only wrapping in a RegExp Like object\n                return { exec: currPattern };\n            }\n            else if (typeof currPattern === \"object\") {\n                hasCustom = true;\n                // ICustomPattern\n                return currPattern;\n            }\n            else if (typeof currPattern === \"string\") {\n                if (currPattern.length === 1) {\n                    return currPattern;\n                }\n                else {\n                    const escapedRegExpString = currPattern.replace(/[\\\\^$.*+?()[\\]{}|]/g, \"\\\\$&\");\n                    const wrappedRegExp = new RegExp(escapedRegExpString);\n                    return options.useSticky\n                        ? addStickyFlag(wrappedRegExp)\n                        : addStartOfInput(wrappedRegExp);\n                }\n            }\n            else {\n                throw Error(\"non exhaustive match\");\n            }\n        });\n    });\n    let patternIdxToType;\n    let patternIdxToGroup;\n    let patternIdxToLongerAltIdxArr;\n    let patternIdxToPushMode;\n    let patternIdxToPopMode;\n    tracer(\"misc mapping\", () => {\n        patternIdxToType = map(onlyRelevantTypes, (currType) => currType.tokenTypeIdx);\n        patternIdxToGroup = map(onlyRelevantTypes, (clazz) => {\n            const groupName = clazz.GROUP;\n            /* istanbul ignore next */\n            if (groupName === Lexer.SKIPPED) {\n                return undefined;\n            }\n            else if (isString(groupName)) {\n                return groupName;\n            }\n            else if (isUndefined(groupName)) {\n                return false;\n            }\n            else {\n                throw Error(\"non exhaustive match\");\n            }\n        });\n        patternIdxToLongerAltIdxArr = map(onlyRelevantTypes, (clazz) => {\n            const longerAltType = clazz.LONGER_ALT;\n            if (longerAltType) {\n                const longerAltIdxArr = isArray(longerAltType)\n                    ? map(longerAltType, (type) => indexOf(onlyRelevantTypes, type))\n                    : [indexOf(onlyRelevantTypes, longerAltType)];\n                return longerAltIdxArr;\n            }\n        });\n        patternIdxToPushMode = map(onlyRelevantTypes, (clazz) => clazz.PUSH_MODE);\n        patternIdxToPopMode = map(onlyRelevantTypes, (clazz) => has(clazz, \"POP_MODE\"));\n    });\n    let patternIdxToCanLineTerminator;\n    tracer(\"Line Terminator Handling\", () => {\n        const lineTerminatorCharCodes = getCharCodes(options.lineTerminatorCharacters);\n        patternIdxToCanLineTerminator = map(onlyRelevantTypes, (tokType) => false);\n        if (options.positionTracking !== \"onlyOffset\") {\n            patternIdxToCanLineTerminator = map(onlyRelevantTypes, (tokType) => {\n                if (has(tokType, \"LINE_BREAKS\")) {\n                    return !!tokType.LINE_BREAKS;\n                }\n                else {\n                    return (checkLineBreaksIssues(tokType, lineTerminatorCharCodes) === false &&\n                        canMatchCharCode(lineTerminatorCharCodes, tokType.PATTERN));\n                }\n            });\n        }\n    });\n    let patternIdxToIsCustom;\n    let patternIdxToShort;\n    let emptyGroups;\n    let patternIdxToConfig;\n    tracer(\"Misc Mapping #2\", () => {\n        patternIdxToIsCustom = map(onlyRelevantTypes, isCustomPattern);\n        patternIdxToShort = map(allTransformedPatterns, isShortPattern);\n        emptyGroups = reduce(onlyRelevantTypes, (acc, clazz) => {\n            const groupName = clazz.GROUP;\n            if (isString(groupName) && !(groupName === Lexer.SKIPPED)) {\n                acc[groupName] = [];\n            }\n            return acc;\n        }, {});\n        patternIdxToConfig = map(allTransformedPatterns, (x, idx) => {\n            return {\n                pattern: allTransformedPatterns[idx],\n                longerAlt: patternIdxToLongerAltIdxArr[idx],\n                canLineTerminator: patternIdxToCanLineTerminator[idx],\n                isCustom: patternIdxToIsCustom[idx],\n                short: patternIdxToShort[idx],\n                group: patternIdxToGroup[idx],\n                push: patternIdxToPushMode[idx],\n                pop: patternIdxToPopMode[idx],\n                tokenTypeIdx: patternIdxToType[idx],\n                tokenType: onlyRelevantTypes[idx],\n            };\n        });\n    });\n    let canBeOptimized = true;\n    let charCodeToPatternIdxToConfig = [];\n    if (!options.safeMode) {\n        tracer(\"First Char Optimization\", () => {\n            charCodeToPatternIdxToConfig = reduce(onlyRelevantTypes, (result, currTokType, idx) => {\n                if (typeof currTokType.PATTERN === \"string\") {\n                    const charCode = currTokType.PATTERN.charCodeAt(0);\n                    const optimizedIdx = charCodeToOptimizedIndex(charCode);\n                    addToMapOfArrays(result, optimizedIdx, patternIdxToConfig[idx]);\n                }\n                else if (isArray(currTokType.START_CHARS_HINT)) {\n                    let lastOptimizedIdx;\n                    forEach(currTokType.START_CHARS_HINT, (charOrInt) => {\n                        const charCode = typeof charOrInt === \"string\"\n                            ? charOrInt.charCodeAt(0)\n                            : charOrInt;\n                        const currOptimizedIdx = charCodeToOptimizedIndex(charCode);\n                        // Avoid adding the config multiple times\n                        /* istanbul ignore else */\n                        // - Difficult to check this scenario effects as it is only a performance\n                        //   optimization that does not change correctness\n                        if (lastOptimizedIdx !== currOptimizedIdx) {\n                            lastOptimizedIdx = currOptimizedIdx;\n                            addToMapOfArrays(result, currOptimizedIdx, patternIdxToConfig[idx]);\n                        }\n                    });\n                }\n                else if (isRegExp(currTokType.PATTERN)) {\n                    if (currTokType.PATTERN.unicode) {\n                        canBeOptimized = false;\n                        if (options.ensureOptimizations) {\n                            PRINT_ERROR(`${failedOptimizationPrefixMsg}` +\n                                `\\tUnable to analyze < ${currTokType.PATTERN.toString()} > pattern.\\n` +\n                                \"\\tThe regexp unicode flag is not currently supported by the regexp-to-ast library.\\n\" +\n                                \"\\tThis will disable the lexer's first char optimizations.\\n\" +\n                                \"\\tFor details See: https://chevrotain.io/docs/guide/resolving_lexer_errors.html#UNICODE_OPTIMIZE\");\n                        }\n                    }\n                    else {\n                        const optimizedCodes = getOptimizedStartCodesIndices(currTokType.PATTERN, options.ensureOptimizations);\n                        /* istanbul ignore if */\n                        // start code will only be empty given an empty regExp or failure of regexp-to-ast library\n                        // the first should be a different validation and the second cannot be tested.\n                        if (isEmpty(optimizedCodes)) {\n                            // we cannot understand what codes may start possible matches\n                            // The optimization correctness requires knowing start codes for ALL patterns.\n                            // Not actually sure this is an error, no debug message\n                            canBeOptimized = false;\n                        }\n                        forEach(optimizedCodes, (code) => {\n                            addToMapOfArrays(result, code, patternIdxToConfig[idx]);\n                        });\n                    }\n                }\n                else {\n                    if (options.ensureOptimizations) {\n                        PRINT_ERROR(`${failedOptimizationPrefixMsg}` +\n                            `\\tTokenType: <${currTokType.name}> is using a custom token pattern without providing <start_chars_hint> parameter.\\n` +\n                            \"\\tThis will disable the lexer's first char optimizations.\\n\" +\n                            \"\\tFor details See: https://chevrotain.io/docs/guide/resolving_lexer_errors.html#CUSTOM_OPTIMIZE\");\n                    }\n                    canBeOptimized = false;\n                }\n                return result;\n            }, []);\n        });\n    }\n    return {\n        emptyGroups: emptyGroups,\n        patternIdxToConfig: patternIdxToConfig,\n        charCodeToPatternIdxToConfig: charCodeToPatternIdxToConfig,\n        hasCustom: hasCustom,\n        canBeOptimized: canBeOptimized,\n    };\n}\nexport function validatePatterns(tokenTypes, validModesNames) {\n    let errors = [];\n    const missingResult = findMissingPatterns(tokenTypes);\n    errors = errors.concat(missingResult.errors);\n    const invalidResult = findInvalidPatterns(missingResult.valid);\n    const validTokenTypes = invalidResult.valid;\n    errors = errors.concat(invalidResult.errors);\n    errors = errors.concat(validateRegExpPattern(validTokenTypes));\n    errors = errors.concat(findInvalidGroupType(validTokenTypes));\n    errors = errors.concat(findModesThatDoNotExist(validTokenTypes, validModesNames));\n    errors = errors.concat(findUnreachablePatterns(validTokenTypes));\n    return errors;\n}\nfunction validateRegExpPattern(tokenTypes) {\n    let errors = [];\n    const withRegExpPatterns = filter(tokenTypes, (currTokType) => isRegExp(currTokType[PATTERN]));\n    errors = errors.concat(findEndOfInputAnchor(withRegExpPatterns));\n    errors = errors.concat(findStartOfInputAnchor(withRegExpPatterns));\n    errors = errors.concat(findUnsupportedFlags(withRegExpPatterns));\n    errors = errors.concat(findDuplicatePatterns(withRegExpPatterns));\n    errors = errors.concat(findEmptyMatchRegExps(withRegExpPatterns));\n    return errors;\n}\nexport function findMissingPatterns(tokenTypes) {\n    const tokenTypesWithMissingPattern = filter(tokenTypes, (currType) => {\n        return !has(currType, PATTERN);\n    });\n    const errors = map(tokenTypesWithMissingPattern, (currType) => {\n        return {\n            message: \"Token Type: ->\" +\n                currType.name +\n                \"<- missing static 'PATTERN' property\",\n            type: LexerDefinitionErrorType.MISSING_PATTERN,\n            tokenTypes: [currType],\n        };\n    });\n    const valid = difference(tokenTypes, tokenTypesWithMissingPattern);\n    return { errors, valid };\n}\nexport function findInvalidPatterns(tokenTypes) {\n    const tokenTypesWithInvalidPattern = filter(tokenTypes, (currType) => {\n        const pattern = currType[PATTERN];\n        return (!isRegExp(pattern) &&\n            !isFunction(pattern) &&\n            !has(pattern, \"exec\") &&\n            !isString(pattern));\n    });\n    const errors = map(tokenTypesWithInvalidPattern, (currType) => {\n        return {\n            message: \"Token Type: ->\" +\n                currType.name +\n                \"<- static 'PATTERN' can only be a RegExp, a\" +\n                \" Function matching the {CustomPatternMatcherFunc} type or an Object matching the {ICustomPattern} interface.\",\n            type: LexerDefinitionErrorType.INVALID_PATTERN,\n            tokenTypes: [currType],\n        };\n    });\n    const valid = difference(tokenTypes, tokenTypesWithInvalidPattern);\n    return { errors, valid };\n}\nconst end_of_input = /[^\\\\][$]/;\nexport function findEndOfInputAnchor(tokenTypes) {\n    class EndAnchorFinder extends BaseRegExpVisitor {\n        constructor() {\n            super(...arguments);\n            this.found = false;\n        }\n        visitEndAnchor(node) {\n            this.found = true;\n        }\n    }\n    const invalidRegex = filter(tokenTypes, (currType) => {\n        const pattern = currType.PATTERN;\n        try {\n            const regexpAst = getRegExpAst(pattern);\n            const endAnchorVisitor = new EndAnchorFinder();\n            endAnchorVisitor.visit(regexpAst);\n            return endAnchorVisitor.found;\n        }\n        catch (e) {\n            // old behavior in case of runtime exceptions with regexp-to-ast.\n            /* istanbul ignore next - cannot ensure an error in regexp-to-ast*/\n            return end_of_input.test(pattern.source);\n        }\n    });\n    const errors = map(invalidRegex, (currType) => {\n        return {\n            message: \"Unexpected RegExp Anchor Error:\\n\" +\n                \"\\tToken Type: ->\" +\n                currType.name +\n                \"<- static 'PATTERN' cannot contain end of input anchor '$'\\n\" +\n                \"\\tSee chevrotain.io/docs/guide/resolving_lexer_errors.html#ANCHORS\" +\n                \"\\tfor details.\",\n            type: LexerDefinitionErrorType.EOI_ANCHOR_FOUND,\n            tokenTypes: [currType],\n        };\n    });\n    return errors;\n}\nexport function findEmptyMatchRegExps(tokenTypes) {\n    const matchesEmptyString = filter(tokenTypes, (currType) => {\n        const pattern = currType.PATTERN;\n        return pattern.test(\"\");\n    });\n    const errors = map(matchesEmptyString, (currType) => {\n        return {\n            message: \"Token Type: ->\" +\n                currType.name +\n                \"<- static 'PATTERN' must not match an empty string\",\n            type: LexerDefinitionErrorType.EMPTY_MATCH_PATTERN,\n            tokenTypes: [currType],\n        };\n    });\n    return errors;\n}\nconst start_of_input = /[^\\\\[][\\^]|^\\^/;\nexport function findStartOfInputAnchor(tokenTypes) {\n    class StartAnchorFinder extends BaseRegExpVisitor {\n        constructor() {\n            super(...arguments);\n            this.found = false;\n        }\n        visitStartAnchor(node) {\n            this.found = true;\n        }\n    }\n    const invalidRegex = filter(tokenTypes, (currType) => {\n        const pattern = currType.PATTERN;\n        try {\n            const regexpAst = getRegExpAst(pattern);\n            const startAnchorVisitor = new StartAnchorFinder();\n            startAnchorVisitor.visit(regexpAst);\n            return startAnchorVisitor.found;\n        }\n        catch (e) {\n            // old behavior in case of runtime exceptions with regexp-to-ast.\n            /* istanbul ignore next - cannot ensure an error in regexp-to-ast*/\n            return start_of_input.test(pattern.source);\n        }\n    });\n    const errors = map(invalidRegex, (currType) => {\n        return {\n            message: \"Unexpected RegExp Anchor Error:\\n\" +\n                \"\\tToken Type: ->\" +\n                currType.name +\n                \"<- static 'PATTERN' cannot contain start of input anchor '^'\\n\" +\n                \"\\tSee https://chevrotain.io/docs/guide/resolving_lexer_errors.html#ANCHORS\" +\n                \"\\tfor details.\",\n            type: LexerDefinitionErrorType.SOI_ANCHOR_FOUND,\n            tokenTypes: [currType],\n        };\n    });\n    return errors;\n}\nexport function findUnsupportedFlags(tokenTypes) {\n    const invalidFlags = filter(tokenTypes, (currType) => {\n        const pattern = currType[PATTERN];\n        return pattern instanceof RegExp && (pattern.multiline || pattern.global);\n    });\n    const errors = map(invalidFlags, (currType) => {\n        return {\n            message: \"Token Type: ->\" +\n                currType.name +\n                \"<- static 'PATTERN' may NOT contain global('g') or multiline('m')\",\n            type: LexerDefinitionErrorType.UNSUPPORTED_FLAGS_FOUND,\n            tokenTypes: [currType],\n        };\n    });\n    return errors;\n}\n// This can only test for identical duplicate RegExps, not semantically equivalent ones.\nexport function findDuplicatePatterns(tokenTypes) {\n    const found = [];\n    let identicalPatterns = map(tokenTypes, (outerType) => {\n        return reduce(tokenTypes, (result, innerType) => {\n            if (outerType.PATTERN.source === innerType.PATTERN.source &&\n                !includes(found, innerType) &&\n                innerType.PATTERN !== Lexer.NA) {\n                // this avoids duplicates in the result, each Token Type may only appear in one \"set\"\n                // in essence we are creating Equivalence classes on equality relation.\n                found.push(innerType);\n                result.push(innerType);\n                return result;\n            }\n            return result;\n        }, []);\n    });\n    identicalPatterns = compact(identicalPatterns);\n    const duplicatePatterns = filter(identicalPatterns, (currIdenticalSet) => {\n        return currIdenticalSet.length > 1;\n    });\n    const errors = map(duplicatePatterns, (setOfIdentical) => {\n        const tokenTypeNames = map(setOfIdentical, (currType) => {\n            return currType.name;\n        });\n        const dupPatternSrc = first(setOfIdentical).PATTERN;\n        return {\n            message: `The same RegExp pattern ->${dupPatternSrc}<-` +\n                `has been used in all of the following Token Types: ${tokenTypeNames.join(\", \")} <-`,\n            type: LexerDefinitionErrorType.DUPLICATE_PATTERNS_FOUND,\n            tokenTypes: setOfIdentical,\n        };\n    });\n    return errors;\n}\nexport function findInvalidGroupType(tokenTypes) {\n    const invalidTypes = filter(tokenTypes, (clazz) => {\n        if (!has(clazz, \"GROUP\")) {\n            return false;\n        }\n        const group = clazz.GROUP;\n        return group !== Lexer.SKIPPED && group !== Lexer.NA && !isString(group);\n    });\n    const errors = map(invalidTypes, (currType) => {\n        return {\n            message: \"Token Type: ->\" +\n                currType.name +\n                \"<- static 'GROUP' can only be Lexer.SKIPPED/Lexer.NA/A String\",\n            type: LexerDefinitionErrorType.INVALID_GROUP_TYPE_FOUND,\n            tokenTypes: [currType],\n        };\n    });\n    return errors;\n}\nexport function findModesThatDoNotExist(tokenTypes, validModes) {\n    const invalidModes = filter(tokenTypes, (clazz) => {\n        return (clazz.PUSH_MODE !== undefined && !includes(validModes, clazz.PUSH_MODE));\n    });\n    const errors = map(invalidModes, (tokType) => {\n        const msg = `Token Type: ->${tokType.name}<- static 'PUSH_MODE' value cannot refer to a Lexer Mode ->${tokType.PUSH_MODE}<-` +\n            `which does not exist`;\n        return {\n            message: msg,\n            type: LexerDefinitionErrorType.PUSH_MODE_DOES_NOT_EXIST,\n            tokenTypes: [tokType],\n        };\n    });\n    return errors;\n}\nexport function findUnreachablePatterns(tokenTypes) {\n    const errors = [];\n    const canBeTested = reduce(tokenTypes, (result, tokType, idx) => {\n        const pattern = tokType.PATTERN;\n        if (pattern === Lexer.NA) {\n            return result;\n        }\n        // a more comprehensive validation for all forms of regExps would require\n        // deeper regExp analysis capabilities\n        if (isString(pattern)) {\n            result.push({ str: pattern, idx, tokenType: tokType });\n        }\n        else if (isRegExp(pattern) && noMetaChar(pattern)) {\n            result.push({ str: pattern.source, idx, tokenType: tokType });\n        }\n        return result;\n    }, []);\n    forEach(tokenTypes, (tokType, testIdx) => {\n        forEach(canBeTested, ({ str, idx, tokenType }) => {\n            if (testIdx < idx && testTokenType(str, tokType.PATTERN)) {\n                const msg = `Token: ->${tokenType.name}<- can never be matched.\\n` +\n                    `Because it appears AFTER the Token Type ->${tokType.name}<-` +\n                    `in the lexer's definition.\\n` +\n                    `See https://chevrotain.io/docs/guide/resolving_lexer_errors.html#UNREACHABLE`;\n                errors.push({\n                    message: msg,\n                    type: LexerDefinitionErrorType.UNREACHABLE_PATTERN,\n                    tokenTypes: [tokType, tokenType],\n                });\n            }\n        });\n    });\n    return errors;\n}\nfunction testTokenType(str, pattern) {\n    /* istanbul ignore else */\n    if (isRegExp(pattern)) {\n        const regExpArray = pattern.exec(str);\n        return regExpArray !== null && regExpArray.index === 0;\n    }\n    else if (isFunction(pattern)) {\n        // maintain the API of custom patterns\n        return pattern(str, 0, [], {});\n    }\n    else if (has(pattern, \"exec\")) {\n        // maintain the API of custom patterns\n        return pattern.exec(str, 0, [], {});\n    }\n    else if (typeof pattern === \"string\") {\n        return pattern === str;\n    }\n    else {\n        throw Error(\"non exhaustive match\");\n    }\n}\nfunction noMetaChar(regExp) {\n    //https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/RegExp\n    const metaChars = [\n        \".\",\n        \"\\\\\",\n        \"[\",\n        \"]\",\n        \"|\",\n        \"^\",\n        \"$\",\n        \"(\",\n        \")\",\n        \"?\",\n        \"*\",\n        \"+\",\n        \"{\",\n    ];\n    return (find(metaChars, (char) => regExp.source.indexOf(char) !== -1) === undefined);\n}\nexport function addStartOfInput(pattern) {\n    const flags = pattern.ignoreCase ? \"i\" : \"\";\n    // always wrapping in a none capturing group preceded by '^' to make sure matching can only work on start of input.\n    // duplicate/redundant start of input markers have no meaning (/^^^^A/ === /^A/)\n    return new RegExp(`^(?:${pattern.source})`, flags);\n}\nexport function addStickyFlag(pattern) {\n    const flags = pattern.ignoreCase ? \"iy\" : \"y\";\n    // always wrapping in a none capturing group preceded by '^' to make sure matching can only work on start of input.\n    // duplicate/redundant start of input markers have no meaning (/^^^^A/ === /^A/)\n    return new RegExp(`${pattern.source}`, flags);\n}\nexport function performRuntimeChecks(lexerDefinition, trackLines, lineTerminatorCharacters) {\n    const errors = [];\n    // some run time checks to help the end users.\n    if (!has(lexerDefinition, DEFAULT_MODE)) {\n        errors.push({\n            message: \"A MultiMode Lexer cannot be initialized without a <\" +\n                DEFAULT_MODE +\n                \"> property in its definition\\n\",\n            type: LexerDefinitionErrorType.MULTI_MODE_LEXER_WITHOUT_DEFAULT_MODE,\n        });\n    }\n    if (!has(lexerDefinition, MODES)) {\n        errors.push({\n            message: \"A MultiMode Lexer cannot be initialized without a <\" +\n                MODES +\n                \"> property in its definition\\n\",\n            type: LexerDefinitionErrorType.MULTI_MODE_LEXER_WITHOUT_MODES_PROPERTY,\n        });\n    }\n    if (has(lexerDefinition, MODES) &&\n        has(lexerDefinition, DEFAULT_MODE) &&\n        !has(lexerDefinition.modes, lexerDefinition.defaultMode)) {\n        errors.push({\n            message: `A MultiMode Lexer cannot be initialized with a ${DEFAULT_MODE}: <${lexerDefinition.defaultMode}>` +\n                `which does not exist\\n`,\n            type: LexerDefinitionErrorType.MULTI_MODE_LEXER_DEFAULT_MODE_VALUE_DOES_NOT_EXIST,\n        });\n    }\n    if (has(lexerDefinition, MODES)) {\n        forEach(lexerDefinition.modes, (currModeValue, currModeName) => {\n            forEach(currModeValue, (currTokType, currIdx) => {\n                if (isUndefined(currTokType)) {\n                    errors.push({\n                        message: `A Lexer cannot be initialized using an undefined Token Type. Mode:` +\n                            `<${currModeName}> at index: <${currIdx}>\\n`,\n                        type: LexerDefinitionErrorType.LEXER_DEFINITION_CANNOT_CONTAIN_UNDEFINED,\n                    });\n                }\n                else if (has(currTokType, \"LONGER_ALT\")) {\n                    const longerAlt = isArray(currTokType.LONGER_ALT)\n                        ? currTokType.LONGER_ALT\n                        : [currTokType.LONGER_ALT];\n                    forEach(longerAlt, (currLongerAlt) => {\n                        if (!isUndefined(currLongerAlt) &&\n                            !includes(currModeValue, currLongerAlt)) {\n                            errors.push({\n                                message: `A MultiMode Lexer cannot be initialized with a longer_alt <${currLongerAlt.name}> on token <${currTokType.name}> outside of mode <${currModeName}>\\n`,\n                                type: LexerDefinitionErrorType.MULTI_MODE_LEXER_LONGER_ALT_NOT_IN_CURRENT_MODE,\n                            });\n                        }\n                    });\n                }\n            });\n        });\n    }\n    return errors;\n}\nexport function performWarningRuntimeChecks(lexerDefinition, trackLines, lineTerminatorCharacters) {\n    const warnings = [];\n    let hasAnyLineBreak = false;\n    const allTokenTypes = compact(flatten(values(lexerDefinition.modes)));\n    const concreteTokenTypes = reject(allTokenTypes, (currType) => currType[PATTERN] === Lexer.NA);\n    const terminatorCharCodes = getCharCodes(lineTerminatorCharacters);\n    if (trackLines) {\n        forEach(concreteTokenTypes, (tokType) => {\n            const currIssue = checkLineBreaksIssues(tokType, terminatorCharCodes);\n            if (currIssue !== false) {\n                const message = buildLineBreakIssueMessage(tokType, currIssue);\n                const warningDescriptor = {\n                    message,\n                    type: currIssue.issue,\n                    tokenType: tokType,\n                };\n                warnings.push(warningDescriptor);\n            }\n            else {\n                // we don't want to attempt to scan if the user explicitly specified the line_breaks option.\n                if (has(tokType, \"LINE_BREAKS\")) {\n                    if (tokType.LINE_BREAKS === true) {\n                        hasAnyLineBreak = true;\n                    }\n                }\n                else {\n                    if (canMatchCharCode(terminatorCharCodes, tokType.PATTERN)) {\n                        hasAnyLineBreak = true;\n                    }\n                }\n            }\n        });\n    }\n    if (trackLines && !hasAnyLineBreak) {\n        warnings.push({\n            message: \"Warning: No LINE_BREAKS Found.\\n\" +\n                \"\\tThis Lexer has been defined to track line and column information,\\n\" +\n                \"\\tBut none of the Token Types can be identified as matching a line terminator.\\n\" +\n                \"\\tSee https://chevrotain.io/docs/guide/resolving_lexer_errors.html#LINE_BREAKS \\n\" +\n                \"\\tfor details.\",\n            type: LexerDefinitionErrorType.NO_LINE_BREAKS_FLAGS,\n        });\n    }\n    return warnings;\n}\nexport function cloneEmptyGroups(emptyGroups) {\n    const clonedResult = {};\n    const groupKeys = keys(emptyGroups);\n    forEach(groupKeys, (currKey) => {\n        const currGroupValue = emptyGroups[currKey];\n        /* istanbul ignore else */\n        if (isArray(currGroupValue)) {\n            clonedResult[currKey] = [];\n        }\n        else {\n            throw Error(\"non exhaustive match\");\n        }\n    });\n    return clonedResult;\n}\n// TODO: refactor to avoid duplication\nexport function isCustomPattern(tokenType) {\n    const pattern = tokenType.PATTERN;\n    /* istanbul ignore else */\n    if (isRegExp(pattern)) {\n        return false;\n    }\n    else if (isFunction(pattern)) {\n        // CustomPatternMatcherFunc - custom patterns do not require any transformations, only wrapping in a RegExp Like object\n        return true;\n    }\n    else if (has(pattern, \"exec\")) {\n        // ICustomPattern\n        return true;\n    }\n    else if (isString(pattern)) {\n        return false;\n    }\n    else {\n        throw Error(\"non exhaustive match\");\n    }\n}\nexport function isShortPattern(pattern) {\n    if (isString(pattern) && pattern.length === 1) {\n        return pattern.charCodeAt(0);\n    }\n    else {\n        return false;\n    }\n}\n/**\n * Faster than using a RegExp for default newline detection during lexing.\n */\nexport const LineTerminatorOptimizedTester = {\n    // implements /\\n|\\r\\n?/g.test\n    test: function (text) {\n        const len = text.length;\n        for (let i = this.lastIndex; i < len; i++) {\n            const c = text.charCodeAt(i);\n            if (c === 10) {\n                this.lastIndex = i + 1;\n                return true;\n            }\n            else if (c === 13) {\n                if (text.charCodeAt(i + 1) === 10) {\n                    this.lastIndex = i + 2;\n                }\n                else {\n                    this.lastIndex = i + 1;\n                }\n                return true;\n            }\n        }\n        return false;\n    },\n    lastIndex: 0,\n};\nfunction checkLineBreaksIssues(tokType, lineTerminatorCharCodes) {\n    if (has(tokType, \"LINE_BREAKS\")) {\n        // if the user explicitly declared the line_breaks option we will respect their choice\n        // and assume it is correct.\n        return false;\n    }\n    else {\n        /* istanbul ignore else */\n        if (isRegExp(tokType.PATTERN)) {\n            try {\n                // TODO: why is the casting suddenly needed?\n                canMatchCharCode(lineTerminatorCharCodes, tokType.PATTERN);\n            }\n            catch (e) {\n                /* istanbul ignore next - to test this we would have to mock <canMatchCharCode> to throw an error */\n                return {\n                    issue: LexerDefinitionErrorType.IDENTIFY_TERMINATOR,\n                    errMsg: e.message,\n                };\n            }\n            return false;\n        }\n        else if (isString(tokType.PATTERN)) {\n            // string literal patterns can always be analyzed to detect line terminator usage\n            return false;\n        }\n        else if (isCustomPattern(tokType)) {\n            // custom token types\n            return { issue: LexerDefinitionErrorType.CUSTOM_LINE_BREAK };\n        }\n        else {\n            throw Error(\"non exhaustive match\");\n        }\n    }\n}\nexport function buildLineBreakIssueMessage(tokType, details) {\n    /* istanbul ignore else */\n    if (details.issue === LexerDefinitionErrorType.IDENTIFY_TERMINATOR) {\n        return (\"Warning: unable to identify line terminator usage in pattern.\\n\" +\n            `\\tThe problem is in the <${tokType.name}> Token Type\\n` +\n            `\\t Root cause: ${details.errMsg}.\\n` +\n            \"\\tFor details See: https://chevrotain.io/docs/guide/resolving_lexer_errors.html#IDENTIFY_TERMINATOR\");\n    }\n    else if (details.issue === LexerDefinitionErrorType.CUSTOM_LINE_BREAK) {\n        return (\"Warning: A Custom Token Pattern should specify the <line_breaks> option.\\n\" +\n            `\\tThe problem is in the <${tokType.name}> Token Type\\n` +\n            \"\\tFor details See: https://chevrotain.io/docs/guide/resolving_lexer_errors.html#CUSTOM_LINE_BREAK\");\n    }\n    else {\n        throw Error(\"non exhaustive match\");\n    }\n}\nfunction getCharCodes(charsOrCodes) {\n    const charCodes = map(charsOrCodes, (numOrString) => {\n        if (isString(numOrString)) {\n            return numOrString.charCodeAt(0);\n        }\n        else {\n            return numOrString;\n        }\n    });\n    return charCodes;\n}\nfunction addToMapOfArrays(map, key, value) {\n    if (map[key] === undefined) {\n        map[key] = [value];\n    }\n    else {\n        map[key].push(value);\n    }\n}\nexport const minOptimizationVal = 256;\n/**\n * We are mapping charCode above ASCI (256) into buckets each in the size of 256.\n * This is because ASCI are the most common start chars so each one of those will get its own\n * possible token configs vector.\n *\n * Tokens starting with charCodes \"above\" ASCI are uncommon, so we can \"afford\"\n * to place these into buckets of possible token configs, What we gain from\n * this is avoiding the case of creating an optimization 'charCodeToPatternIdxToConfig'\n * which would contain 10,000+ arrays of small size (e.g unicode Identifiers scenario).\n * Our 'charCodeToPatternIdxToConfig' max size will now be:\n * 256 + (2^16 / 2^8) - 1 === 511\n *\n * note the hack for fast division integer part extraction\n * See: https://stackoverflow.com/a/4228528\n */\nlet charCodeToOptimizedIdxMap = [];\nexport function charCodeToOptimizedIndex(charCode) {\n    return charCode < minOptimizationVal\n        ? charCode\n        : charCodeToOptimizedIdxMap[charCode];\n}\n/**\n * This is a compromise between cold start / hot running performance\n * Creating this array takes ~3ms on a modern machine,\n * But if we perform the computation at runtime as needed the CSS Lexer benchmark\n * performance degrades by ~10%\n *\n * TODO: Perhaps it should be lazy initialized only if a charCode > 255 is used.\n */\nfunction initCharCodeToOptimizedIndexMap() {\n    if (isEmpty(charCodeToOptimizedIdxMap)) {\n        charCodeToOptimizedIdxMap = new Array(65536);\n        for (let i = 0; i < 65536; i++) {\n            charCodeToOptimizedIdxMap[i] = i > 255 ? 255 + ~~(i / 255) : i;\n        }\n    }\n}\n//# sourceMappingURL=lexer.js.map","import { analyzeTokenTypes, charCodeToOptimizedIndex, cloneEmptyGroups, DEFAULT_MODE, LineTerminatorOptimizedTester, performRuntimeChecks, performWarningRuntimeChecks, SUPPORT_STICKY, validatePatterns, } from \"./lexer.js\";\nimport { assign, clone, forEach, identity, isArray, isEmpty, isUndefined, keys, last, map, noop, reduce, reject, } from \"lodash-es\";\nimport { PRINT_WARNING, timer, toFastProperties } from \"@chevrotain/utils\";\nimport { augmentTokenTypes } from \"./tokens.js\";\nimport { defaultLexerErrorProvider } from \"./lexer_errors_public.js\";\nimport { clearRegExpParserCache } from \"./reg_exp_parser.js\";\nexport var LexerDefinitionErrorType;\n(function (LexerDefinitionErrorType) {\n    LexerDefinitionErrorType[LexerDefinitionErrorType[\"MISSING_PATTERN\"] = 0] = \"MISSING_PATTERN\";\n    LexerDefinitionErrorType[LexerDefinitionErrorType[\"INVALID_PATTERN\"] = 1] = \"INVALID_PATTERN\";\n    LexerDefinitionErrorType[LexerDefinitionErrorType[\"EOI_ANCHOR_FOUND\"] = 2] = \"EOI_ANCHOR_FOUND\";\n    LexerDefinitionErrorType[LexerDefinitionErrorType[\"UNSUPPORTED_FLAGS_FOUND\"] = 3] = \"UNSUPPORTED_FLAGS_FOUND\";\n    LexerDefinitionErrorType[LexerDefinitionErrorType[\"DUPLICATE_PATTERNS_FOUND\"] = 4] = \"DUPLICATE_PATTERNS_FOUND\";\n    LexerDefinitionErrorType[LexerDefinitionErrorType[\"INVALID_GROUP_TYPE_FOUND\"] = 5] = \"INVALID_GROUP_TYPE_FOUND\";\n    LexerDefinitionErrorType[LexerDefinitionErrorType[\"PUSH_MODE_DOES_NOT_EXIST\"] = 6] = \"PUSH_MODE_DOES_NOT_EXIST\";\n    LexerDefinitionErrorType[LexerDefinitionErrorType[\"MULTI_MODE_LEXER_WITHOUT_DEFAULT_MODE\"] = 7] = \"MULTI_MODE_LEXER_WITHOUT_DEFAULT_MODE\";\n    LexerDefinitionErrorType[LexerDefinitionErrorType[\"MULTI_MODE_LEXER_WITHOUT_MODES_PROPERTY\"] = 8] = \"MULTI_MODE_LEXER_WITHOUT_MODES_PROPERTY\";\n    LexerDefinitionErrorType[LexerDefinitionErrorType[\"MULTI_MODE_LEXER_DEFAULT_MODE_VALUE_DOES_NOT_EXIST\"] = 9] = \"MULTI_MODE_LEXER_DEFAULT_MODE_VALUE_DOES_NOT_EXIST\";\n    LexerDefinitionErrorType[LexerDefinitionErrorType[\"LEXER_DEFINITION_CANNOT_CONTAIN_UNDEFINED\"] = 10] = \"LEXER_DEFINITION_CANNOT_CONTAIN_UNDEFINED\";\n    LexerDefinitionErrorType[LexerDefinitionErrorType[\"SOI_ANCHOR_FOUND\"] = 11] = \"SOI_ANCHOR_FOUND\";\n    LexerDefinitionErrorType[LexerDefinitionErrorType[\"EMPTY_MATCH_PATTERN\"] = 12] = \"EMPTY_MATCH_PATTERN\";\n    LexerDefinitionErrorType[LexerDefinitionErrorType[\"NO_LINE_BREAKS_FLAGS\"] = 13] = \"NO_LINE_BREAKS_FLAGS\";\n    LexerDefinitionErrorType[LexerDefinitionErrorType[\"UNREACHABLE_PATTERN\"] = 14] = \"UNREACHABLE_PATTERN\";\n    LexerDefinitionErrorType[LexerDefinitionErrorType[\"IDENTIFY_TERMINATOR\"] = 15] = \"IDENTIFY_TERMINATOR\";\n    LexerDefinitionErrorType[LexerDefinitionErrorType[\"CUSTOM_LINE_BREAK\"] = 16] = \"CUSTOM_LINE_BREAK\";\n    LexerDefinitionErrorType[LexerDefinitionErrorType[\"MULTI_MODE_LEXER_LONGER_ALT_NOT_IN_CURRENT_MODE\"] = 17] = \"MULTI_MODE_LEXER_LONGER_ALT_NOT_IN_CURRENT_MODE\";\n})(LexerDefinitionErrorType || (LexerDefinitionErrorType = {}));\nconst DEFAULT_LEXER_CONFIG = {\n    deferDefinitionErrorsHandling: false,\n    positionTracking: \"full\",\n    lineTerminatorsPattern: /\\n|\\r\\n?/g,\n    lineTerminatorCharacters: [\"\\n\", \"\\r\"],\n    ensureOptimizations: false,\n    safeMode: false,\n    errorMessageProvider: defaultLexerErrorProvider,\n    traceInitPerf: false,\n    skipValidations: false,\n    recoveryEnabled: true,\n};\nObject.freeze(DEFAULT_LEXER_CONFIG);\nexport class Lexer {\n    constructor(lexerDefinition, config = DEFAULT_LEXER_CONFIG) {\n        this.lexerDefinition = lexerDefinition;\n        this.lexerDefinitionErrors = [];\n        this.lexerDefinitionWarning = [];\n        this.patternIdxToConfig = {};\n        this.charCodeToPatternIdxToConfig = {};\n        this.modes = [];\n        this.emptyGroups = {};\n        this.trackStartLines = true;\n        this.trackEndLines = true;\n        this.hasCustom = false;\n        this.canModeBeOptimized = {};\n        // Duplicated from the parser's perf trace trait to allow future extraction\n        // of the lexer to a separate package.\n        this.TRACE_INIT = (phaseDesc, phaseImpl) => {\n            // No need to optimize this using NOOP pattern because\n            // It is not called in a hot spot...\n            if (this.traceInitPerf === true) {\n                this.traceInitIndent++;\n                const indent = new Array(this.traceInitIndent + 1).join(\"\\t\");\n                if (this.traceInitIndent < this.traceInitMaxIdent) {\n                    console.log(`${indent}--> <${phaseDesc}>`);\n                }\n                const { time, value } = timer(phaseImpl);\n                /* istanbul ignore next - Difficult to reproduce specific performance behavior (>10ms) in tests */\n                const traceMethod = time > 10 ? console.warn : console.log;\n                if (this.traceInitIndent < this.traceInitMaxIdent) {\n                    traceMethod(`${indent}<-- <${phaseDesc}> time: ${time}ms`);\n                }\n                this.traceInitIndent--;\n                return value;\n            }\n            else {\n                return phaseImpl();\n            }\n        };\n        if (typeof config === \"boolean\") {\n            throw Error(\"The second argument to the Lexer constructor is now an ILexerConfig Object.\\n\" +\n                \"a boolean 2nd argument is no longer supported\");\n        }\n        // todo: defaults func?\n        this.config = assign({}, DEFAULT_LEXER_CONFIG, config);\n        const traceInitVal = this.config.traceInitPerf;\n        if (traceInitVal === true) {\n            this.traceInitMaxIdent = Infinity;\n            this.traceInitPerf = true;\n        }\n        else if (typeof traceInitVal === \"number\") {\n            this.traceInitMaxIdent = traceInitVal;\n            this.traceInitPerf = true;\n        }\n        this.traceInitIndent = -1;\n        this.TRACE_INIT(\"Lexer Constructor\", () => {\n            let actualDefinition;\n            let hasOnlySingleMode = true;\n            this.TRACE_INIT(\"Lexer Config handling\", () => {\n                if (this.config.lineTerminatorsPattern ===\n                    DEFAULT_LEXER_CONFIG.lineTerminatorsPattern) {\n                    // optimized built-in implementation for the defaults definition of lineTerminators\n                    this.config.lineTerminatorsPattern = LineTerminatorOptimizedTester;\n                }\n                else {\n                    if (this.config.lineTerminatorCharacters ===\n                        DEFAULT_LEXER_CONFIG.lineTerminatorCharacters) {\n                        throw Error(\"Error: Missing <lineTerminatorCharacters> property on the Lexer config.\\n\" +\n                            \"\\tFor details See: https://chevrotain.io/docs/guide/resolving_lexer_errors.html#MISSING_LINE_TERM_CHARS\");\n                    }\n                }\n                if (config.safeMode && config.ensureOptimizations) {\n                    throw Error('\"safeMode\" and \"ensureOptimizations\" flags are mutually exclusive.');\n                }\n                this.trackStartLines = /full|onlyStart/i.test(this.config.positionTracking);\n                this.trackEndLines = /full/i.test(this.config.positionTracking);\n                // Convert SingleModeLexerDefinition into a IMultiModeLexerDefinition.\n                if (isArray(lexerDefinition)) {\n                    actualDefinition = {\n                        modes: { defaultMode: clone(lexerDefinition) },\n                        defaultMode: DEFAULT_MODE,\n                    };\n                }\n                else {\n                    // no conversion needed, input should already be a IMultiModeLexerDefinition\n                    hasOnlySingleMode = false;\n                    actualDefinition = clone(lexerDefinition);\n                }\n            });\n            if (this.config.skipValidations === false) {\n                this.TRACE_INIT(\"performRuntimeChecks\", () => {\n                    this.lexerDefinitionErrors = this.lexerDefinitionErrors.concat(performRuntimeChecks(actualDefinition, this.trackStartLines, this.config.lineTerminatorCharacters));\n                });\n                this.TRACE_INIT(\"performWarningRuntimeChecks\", () => {\n                    this.lexerDefinitionWarning = this.lexerDefinitionWarning.concat(performWarningRuntimeChecks(actualDefinition, this.trackStartLines, this.config.lineTerminatorCharacters));\n                });\n            }\n            // for extra robustness to avoid throwing an none informative error message\n            actualDefinition.modes = actualDefinition.modes\n                ? actualDefinition.modes\n                : {};\n            // an error of undefined TokenTypes will be detected in \"performRuntimeChecks\" above.\n            // this transformation is to increase robustness in the case of partially invalid lexer definition.\n            forEach(actualDefinition.modes, (currModeValue, currModeName) => {\n                actualDefinition.modes[currModeName] = reject(currModeValue, (currTokType) => isUndefined(currTokType));\n            });\n            const allModeNames = keys(actualDefinition.modes);\n            forEach(actualDefinition.modes, (currModDef, currModName) => {\n                this.TRACE_INIT(`Mode: <${currModName}> processing`, () => {\n                    this.modes.push(currModName);\n                    if (this.config.skipValidations === false) {\n                        this.TRACE_INIT(`validatePatterns`, () => {\n                            this.lexerDefinitionErrors = this.lexerDefinitionErrors.concat(validatePatterns(currModDef, allModeNames));\n                        });\n                    }\n                    // If definition errors were encountered, the analysis phase may fail unexpectedly/\n                    // Considering a lexer with definition errors may never be used, there is no point\n                    // to performing the analysis anyhow...\n                    if (isEmpty(this.lexerDefinitionErrors)) {\n                        augmentTokenTypes(currModDef);\n                        let currAnalyzeResult;\n                        this.TRACE_INIT(`analyzeTokenTypes`, () => {\n                            currAnalyzeResult = analyzeTokenTypes(currModDef, {\n                                lineTerminatorCharacters: this.config.lineTerminatorCharacters,\n                                positionTracking: config.positionTracking,\n                                ensureOptimizations: config.ensureOptimizations,\n                                safeMode: config.safeMode,\n                                tracer: this.TRACE_INIT,\n                            });\n                        });\n                        this.patternIdxToConfig[currModName] =\n                            currAnalyzeResult.patternIdxToConfig;\n                        this.charCodeToPatternIdxToConfig[currModName] =\n                            currAnalyzeResult.charCodeToPatternIdxToConfig;\n                        this.emptyGroups = assign({}, this.emptyGroups, currAnalyzeResult.emptyGroups);\n                        this.hasCustom = currAnalyzeResult.hasCustom || this.hasCustom;\n                        this.canModeBeOptimized[currModName] =\n                            currAnalyzeResult.canBeOptimized;\n                    }\n                });\n            });\n            this.defaultMode = actualDefinition.defaultMode;\n            if (!isEmpty(this.lexerDefinitionErrors) &&\n                !this.config.deferDefinitionErrorsHandling) {\n                const allErrMessages = map(this.lexerDefinitionErrors, (error) => {\n                    return error.message;\n                });\n                const allErrMessagesString = allErrMessages.join(\"-----------------------\\n\");\n                throw new Error(\"Errors detected in definition of Lexer:\\n\" + allErrMessagesString);\n            }\n            // Only print warning if there are no errors, This will avoid pl\n            forEach(this.lexerDefinitionWarning, (warningDescriptor) => {\n                PRINT_WARNING(warningDescriptor.message);\n            });\n            this.TRACE_INIT(\"Choosing sub-methods implementations\", () => {\n                // Choose the relevant internal implementations for this specific parser.\n                // These implementations should be in-lined by the JavaScript engine\n                // to provide optimal performance in each scenario.\n                if (SUPPORT_STICKY) {\n                    this.chopInput = identity;\n                    this.match = this.matchWithTest;\n                }\n                else {\n                    this.updateLastIndex = noop;\n                    this.match = this.matchWithExec;\n                }\n                if (hasOnlySingleMode) {\n                    this.handleModes = noop;\n                }\n                if (this.trackStartLines === false) {\n                    this.computeNewColumn = identity;\n                }\n                if (this.trackEndLines === false) {\n                    this.updateTokenEndLineColumnLocation = noop;\n                }\n                if (/full/i.test(this.config.positionTracking)) {\n                    this.createTokenInstance = this.createFullToken;\n                }\n                else if (/onlyStart/i.test(this.config.positionTracking)) {\n                    this.createTokenInstance = this.createStartOnlyToken;\n                }\n                else if (/onlyOffset/i.test(this.config.positionTracking)) {\n                    this.createTokenInstance = this.createOffsetOnlyToken;\n                }\n                else {\n                    throw Error(`Invalid <positionTracking> config option: \"${this.config.positionTracking}\"`);\n                }\n                if (this.hasCustom) {\n                    this.addToken = this.addTokenUsingPush;\n                    this.handlePayload = this.handlePayloadWithCustom;\n                }\n                else {\n                    this.addToken = this.addTokenUsingMemberAccess;\n                    this.handlePayload = this.handlePayloadNoCustom;\n                }\n            });\n            this.TRACE_INIT(\"Failed Optimization Warnings\", () => {\n                const unOptimizedModes = reduce(this.canModeBeOptimized, (cannotBeOptimized, canBeOptimized, modeName) => {\n                    if (canBeOptimized === false) {\n                        cannotBeOptimized.push(modeName);\n                    }\n                    return cannotBeOptimized;\n                }, []);\n                if (config.ensureOptimizations && !isEmpty(unOptimizedModes)) {\n                    throw Error(`Lexer Modes: < ${unOptimizedModes.join(\", \")} > cannot be optimized.\\n` +\n                        '\\t Disable the \"ensureOptimizations\" lexer config flag to silently ignore this and run the lexer in an un-optimized mode.\\n' +\n                        \"\\t Or inspect the console log for details on how to resolve these issues.\");\n                }\n            });\n            this.TRACE_INIT(\"clearRegExpParserCache\", () => {\n                clearRegExpParserCache();\n            });\n            this.TRACE_INIT(\"toFastProperties\", () => {\n                toFastProperties(this);\n            });\n        });\n    }\n    tokenize(text, initialMode = this.defaultMode) {\n        if (!isEmpty(this.lexerDefinitionErrors)) {\n            const allErrMessages = map(this.lexerDefinitionErrors, (error) => {\n                return error.message;\n            });\n            const allErrMessagesString = allErrMessages.join(\"-----------------------\\n\");\n            throw new Error(\"Unable to Tokenize because Errors detected in definition of Lexer:\\n\" +\n                allErrMessagesString);\n        }\n        return this.tokenizeInternal(text, initialMode);\n    }\n    // There is quite a bit of duplication between this and \"tokenizeInternalLazy\"\n    // This is intentional due to performance considerations.\n    // this method also used quite a bit of `!` none null assertions because it is too optimized\n    // for `tsc` to always understand it is \"safe\"\n    tokenizeInternal(text, initialMode) {\n        let i, j, k, matchAltImage, longerAlt, matchedImage, payload, altPayload, imageLength, group, tokType, newToken, errLength, droppedChar, msg, match;\n        const orgText = text;\n        const orgLength = orgText.length;\n        let offset = 0;\n        let matchedTokensIndex = 0;\n        // initializing the tokensArray to the \"guessed\" size.\n        // guessing too little will still reduce the number of array re-sizes on pushes.\n        // guessing too large (Tested by guessing x4 too large) may cost a bit more of memory\n        // but would still have a faster runtime by avoiding (All but one) array resizing.\n        const guessedNumberOfTokens = this.hasCustom\n            ? 0 // will break custom token pattern APIs the matchedTokens array will contain undefined elements.\n            : Math.floor(text.length / 10);\n        const matchedTokens = new Array(guessedNumberOfTokens);\n        const errors = [];\n        let line = this.trackStartLines ? 1 : undefined;\n        let column = this.trackStartLines ? 1 : undefined;\n        const groups = cloneEmptyGroups(this.emptyGroups);\n        const trackLines = this.trackStartLines;\n        const lineTerminatorPattern = this.config.lineTerminatorsPattern;\n        let currModePatternsLength = 0;\n        let patternIdxToConfig = [];\n        let currCharCodeToPatternIdxToConfig = [];\n        const modeStack = [];\n        const emptyArray = [];\n        Object.freeze(emptyArray);\n        let getPossiblePatterns;\n        function getPossiblePatternsSlow() {\n            return patternIdxToConfig;\n        }\n        function getPossiblePatternsOptimized(charCode) {\n            const optimizedCharIdx = charCodeToOptimizedIndex(charCode);\n            const possiblePatterns = currCharCodeToPatternIdxToConfig[optimizedCharIdx];\n            if (possiblePatterns === undefined) {\n                return emptyArray;\n            }\n            else {\n                return possiblePatterns;\n            }\n        }\n        const pop_mode = (popToken) => {\n            // TODO: perhaps avoid this error in the edge case there is no more input?\n            if (modeStack.length === 1 &&\n                // if we have both a POP_MODE and a PUSH_MODE this is in-fact a \"transition\"\n                // So no error should occur.\n                popToken.tokenType.PUSH_MODE === undefined) {\n                // if we try to pop the last mode there lexer will no longer have ANY mode.\n                // thus the pop is ignored, an error will be created and the lexer will continue parsing in the previous mode.\n                const msg = this.config.errorMessageProvider.buildUnableToPopLexerModeMessage(popToken);\n                errors.push({\n                    offset: popToken.startOffset,\n                    line: popToken.startLine,\n                    column: popToken.startColumn,\n                    length: popToken.image.length,\n                    message: msg,\n                });\n            }\n            else {\n                modeStack.pop();\n                const newMode = last(modeStack);\n                patternIdxToConfig = this.patternIdxToConfig[newMode];\n                currCharCodeToPatternIdxToConfig =\n                    this.charCodeToPatternIdxToConfig[newMode];\n                currModePatternsLength = patternIdxToConfig.length;\n                const modeCanBeOptimized = this.canModeBeOptimized[newMode] && this.config.safeMode === false;\n                if (currCharCodeToPatternIdxToConfig && modeCanBeOptimized) {\n                    getPossiblePatterns = getPossiblePatternsOptimized;\n                }\n                else {\n                    getPossiblePatterns = getPossiblePatternsSlow;\n                }\n            }\n        };\n        function push_mode(newMode) {\n            modeStack.push(newMode);\n            currCharCodeToPatternIdxToConfig =\n                this.charCodeToPatternIdxToConfig[newMode];\n            patternIdxToConfig = this.patternIdxToConfig[newMode];\n            currModePatternsLength = patternIdxToConfig.length;\n            currModePatternsLength = patternIdxToConfig.length;\n            const modeCanBeOptimized = this.canModeBeOptimized[newMode] && this.config.safeMode === false;\n            if (currCharCodeToPatternIdxToConfig && modeCanBeOptimized) {\n                getPossiblePatterns = getPossiblePatternsOptimized;\n            }\n            else {\n                getPossiblePatterns = getPossiblePatternsSlow;\n            }\n        }\n        // this pattern seems to avoid a V8 de-optimization, although that de-optimization does not\n        // seem to matter performance wise.\n        push_mode.call(this, initialMode);\n        let currConfig;\n        const recoveryEnabled = this.config.recoveryEnabled;\n        while (offset < orgLength) {\n            matchedImage = null;\n            const nextCharCode = orgText.charCodeAt(offset);\n            const chosenPatternIdxToConfig = getPossiblePatterns(nextCharCode);\n            const chosenPatternsLength = chosenPatternIdxToConfig.length;\n            for (i = 0; i < chosenPatternsLength; i++) {\n                currConfig = chosenPatternIdxToConfig[i];\n                const currPattern = currConfig.pattern;\n                payload = null;\n                // manually in-lined because > 600 chars won't be in-lined in V8\n                const singleCharCode = currConfig.short;\n                if (singleCharCode !== false) {\n                    if (nextCharCode === singleCharCode) {\n                        // single character string\n                        matchedImage = currPattern;\n                    }\n                }\n                else if (currConfig.isCustom === true) {\n                    match = currPattern.exec(orgText, offset, matchedTokens, groups);\n                    if (match !== null) {\n                        matchedImage = match[0];\n                        if (match.payload !== undefined) {\n                            payload = match.payload;\n                        }\n                    }\n                    else {\n                        matchedImage = null;\n                    }\n                }\n                else {\n                    this.updateLastIndex(currPattern, offset);\n                    matchedImage = this.match(currPattern, text, offset);\n                }\n                if (matchedImage !== null) {\n                    // even though this pattern matched we must try a another longer alternative.\n                    // this can be used to prioritize keywords over identifiers\n                    longerAlt = currConfig.longerAlt;\n                    if (longerAlt !== undefined) {\n                        // TODO: micro optimize, avoid extra prop access\n                        // by saving/linking longerAlt on the original config?\n                        const longerAltLength = longerAlt.length;\n                        for (k = 0; k < longerAltLength; k++) {\n                            const longerAltConfig = patternIdxToConfig[longerAlt[k]];\n                            const longerAltPattern = longerAltConfig.pattern;\n                            altPayload = null;\n                            // single Char can never be a longer alt so no need to test it.\n                            // manually in-lined because > 600 chars won't be in-lined in V8\n                            if (longerAltConfig.isCustom === true) {\n                                match = longerAltPattern.exec(orgText, offset, matchedTokens, groups);\n                                if (match !== null) {\n                                    matchAltImage = match[0];\n                                    if (match.payload !== undefined) {\n                                        altPayload = match.payload;\n                                    }\n                                }\n                                else {\n                                    matchAltImage = null;\n                                }\n                            }\n                            else {\n                                this.updateLastIndex(longerAltPattern, offset);\n                                matchAltImage = this.match(longerAltPattern, text, offset);\n                            }\n                            if (matchAltImage && matchAltImage.length > matchedImage.length) {\n                                matchedImage = matchAltImage;\n                                payload = altPayload;\n                                currConfig = longerAltConfig;\n                                // Exit the loop early after matching one of the longer alternatives\n                                // The first matched alternative takes precedence\n                                break;\n                            }\n                        }\n                    }\n                    break;\n                }\n            }\n            // successful match\n            if (matchedImage !== null) {\n                imageLength = matchedImage.length;\n                group = currConfig.group;\n                if (group !== undefined) {\n                    tokType = currConfig.tokenTypeIdx;\n                    // TODO: \"offset + imageLength\" and the new column may be computed twice in case of \"full\" location information inside\n                    // createFullToken method\n                    newToken = this.createTokenInstance(matchedImage, offset, tokType, currConfig.tokenType, line, column, imageLength);\n                    this.handlePayload(newToken, payload);\n                    // TODO: optimize NOOP in case there are no special groups?\n                    if (group === false) {\n                        matchedTokensIndex = this.addToken(matchedTokens, matchedTokensIndex, newToken);\n                    }\n                    else {\n                        groups[group].push(newToken);\n                    }\n                }\n                text = this.chopInput(text, imageLength);\n                offset = offset + imageLength;\n                // TODO: with newlines the column may be assigned twice\n                column = this.computeNewColumn(column, imageLength);\n                if (trackLines === true && currConfig.canLineTerminator === true) {\n                    let numOfLTsInMatch = 0;\n                    let foundTerminator;\n                    let lastLTEndOffset;\n                    lineTerminatorPattern.lastIndex = 0;\n                    do {\n                        foundTerminator = lineTerminatorPattern.test(matchedImage);\n                        if (foundTerminator === true) {\n                            lastLTEndOffset = lineTerminatorPattern.lastIndex - 1;\n                            numOfLTsInMatch++;\n                        }\n                    } while (foundTerminator === true);\n                    if (numOfLTsInMatch !== 0) {\n                        line = line + numOfLTsInMatch;\n                        column = imageLength - lastLTEndOffset;\n                        this.updateTokenEndLineColumnLocation(newToken, group, lastLTEndOffset, numOfLTsInMatch, line, column, imageLength);\n                    }\n                }\n                // will be NOOP if no modes present\n                this.handleModes(currConfig, pop_mode, push_mode, newToken);\n            }\n            else {\n                // error recovery, drop characters until we identify a valid token's start point\n                const errorStartOffset = offset;\n                const errorLine = line;\n                const errorColumn = column;\n                let foundResyncPoint = recoveryEnabled === false;\n                while (foundResyncPoint === false && offset < orgLength) {\n                    // Identity Func (when sticky flag is enabled)\n                    text = this.chopInput(text, 1);\n                    offset++;\n                    for (j = 0; j < currModePatternsLength; j++) {\n                        const currConfig = patternIdxToConfig[j];\n                        const currPattern = currConfig.pattern;\n                        // manually in-lined because > 600 chars won't be in-lined in V8\n                        const singleCharCode = currConfig.short;\n                        if (singleCharCode !== false) {\n                            if (orgText.charCodeAt(offset) === singleCharCode) {\n                                // single character string\n                                foundResyncPoint = true;\n                            }\n                        }\n                        else if (currConfig.isCustom === true) {\n                            foundResyncPoint =\n                                currPattern.exec(orgText, offset, matchedTokens, groups) !== null;\n                        }\n                        else {\n                            this.updateLastIndex(currPattern, offset);\n                            foundResyncPoint = currPattern.exec(text) !== null;\n                        }\n                        if (foundResyncPoint === true) {\n                            break;\n                        }\n                    }\n                }\n                errLength = offset - errorStartOffset;\n                column = this.computeNewColumn(column, errLength);\n                // at this point we either re-synced or reached the end of the input text\n                msg = this.config.errorMessageProvider.buildUnexpectedCharactersMessage(orgText, errorStartOffset, errLength, errorLine, errorColumn);\n                errors.push({\n                    offset: errorStartOffset,\n                    line: errorLine,\n                    column: errorColumn,\n                    length: errLength,\n                    message: msg,\n                });\n                if (recoveryEnabled === false) {\n                    break;\n                }\n            }\n        }\n        // if we do have custom patterns which push directly into the\n        // TODO: custom tokens should not push directly??\n        if (!this.hasCustom) {\n            // if we guessed a too large size for the tokens array this will shrink it to the right size.\n            matchedTokens.length = matchedTokensIndex;\n        }\n        return {\n            tokens: matchedTokens,\n            groups: groups,\n            errors: errors,\n        };\n    }\n    handleModes(config, pop_mode, push_mode, newToken) {\n        if (config.pop === true) {\n            // need to save the PUSH_MODE property as if the mode is popped\n            // patternIdxToPopMode is updated to reflect the new mode after popping the stack\n            const pushMode = config.push;\n            pop_mode(newToken);\n            if (pushMode !== undefined) {\n                push_mode.call(this, pushMode);\n            }\n        }\n        else if (config.push !== undefined) {\n            push_mode.call(this, config.push);\n        }\n    }\n    chopInput(text, length) {\n        return text.substring(length);\n    }\n    updateLastIndex(regExp, newLastIndex) {\n        regExp.lastIndex = newLastIndex;\n    }\n    // TODO: decrease this under 600 characters? inspect stripping comments option in TSC compiler\n    updateTokenEndLineColumnLocation(newToken, group, lastLTIdx, numOfLTsInMatch, line, column, imageLength) {\n        let lastCharIsLT, fixForEndingInLT;\n        if (group !== undefined) {\n            // a none skipped multi line Token, need to update endLine/endColumn\n            lastCharIsLT = lastLTIdx === imageLength - 1;\n            fixForEndingInLT = lastCharIsLT ? -1 : 0;\n            if (!(numOfLTsInMatch === 1 && lastCharIsLT === true)) {\n                // if a token ends in a LT that last LT only affects the line numbering of following Tokens\n                newToken.endLine = line + fixForEndingInLT;\n                // the last LT in a token does not affect the endColumn either as the [columnStart ... columnEnd)\n                // inclusive to exclusive range.\n                newToken.endColumn = column - 1 + -fixForEndingInLT;\n            }\n            // else single LT in the last character of a token, no need to modify the endLine/EndColumn\n        }\n    }\n    computeNewColumn(oldColumn, imageLength) {\n        return oldColumn + imageLength;\n    }\n    createOffsetOnlyToken(image, startOffset, tokenTypeIdx, tokenType) {\n        return {\n            image,\n            startOffset,\n            tokenTypeIdx,\n            tokenType,\n        };\n    }\n    createStartOnlyToken(image, startOffset, tokenTypeIdx, tokenType, startLine, startColumn) {\n        return {\n            image,\n            startOffset,\n            startLine,\n            startColumn,\n            tokenTypeIdx,\n            tokenType,\n        };\n    }\n    createFullToken(image, startOffset, tokenTypeIdx, tokenType, startLine, startColumn, imageLength) {\n        return {\n            image,\n            startOffset,\n            endOffset: startOffset + imageLength - 1,\n            startLine,\n            endLine: startLine,\n            startColumn,\n            endColumn: startColumn + imageLength - 1,\n            tokenTypeIdx,\n            tokenType,\n        };\n    }\n    addTokenUsingPush(tokenVector, index, tokenToAdd) {\n        tokenVector.push(tokenToAdd);\n        return index;\n    }\n    addTokenUsingMemberAccess(tokenVector, index, tokenToAdd) {\n        tokenVector[index] = tokenToAdd;\n        index++;\n        return index;\n    }\n    handlePayloadNoCustom(token, payload) { }\n    handlePayloadWithCustom(token, payload) {\n        if (payload !== null) {\n            token.payload = payload;\n        }\n    }\n    matchWithTest(pattern, text, offset) {\n        const found = pattern.test(text);\n        if (found === true) {\n            return text.substring(offset, pattern.lastIndex);\n        }\n        return null;\n    }\n    matchWithExec(pattern, text) {\n        const regExpArray = pattern.exec(text);\n        return regExpArray !== null ? regExpArray[0] : null;\n    }\n}\nLexer.SKIPPED = \"This marks a skipped Token pattern, this means each token identified by it will\" +\n    \"be consumed and then thrown into oblivion, this can be used to for example to completely ignore whitespace.\";\nLexer.NA = /NOT_APPLICABLE/;\n//# sourceMappingURL=lexer_public.js.map","import { clone, compact, difference, flatten, forEach, has, includes, isArray, isEmpty, map, } from \"lodash-es\";\nexport function tokenStructuredMatcher(tokInstance, tokConstructor) {\n    const instanceType = tokInstance.tokenTypeIdx;\n    if (instanceType === tokConstructor.tokenTypeIdx) {\n        return true;\n    }\n    else {\n        return (tokConstructor.isParent === true &&\n            tokConstructor.categoryMatchesMap[instanceType] === true);\n    }\n}\n// Optimized tokenMatcher in case our grammar does not use token categories\n// Being so tiny it is much more likely to be in-lined and this avoid the function call overhead\nexport function tokenStructuredMatcherNoCategories(token, tokType) {\n    return token.tokenTypeIdx === tokType.tokenTypeIdx;\n}\nexport let tokenShortNameIdx = 1;\nexport const tokenIdxToClass = {};\nexport function augmentTokenTypes(tokenTypes) {\n    // collect the parent Token Types as well.\n    const tokenTypesAndParents = expandCategories(tokenTypes);\n    // add required tokenType and categoryMatches properties\n    assignTokenDefaultProps(tokenTypesAndParents);\n    // fill up the categoryMatches\n    assignCategoriesMapProp(tokenTypesAndParents);\n    assignCategoriesTokensProp(tokenTypesAndParents);\n    forEach(tokenTypesAndParents, (tokType) => {\n        tokType.isParent = tokType.categoryMatches.length > 0;\n    });\n}\nexport function expandCategories(tokenTypes) {\n    let result = clone(tokenTypes);\n    let categories = tokenTypes;\n    let searching = true;\n    while (searching) {\n        categories = compact(flatten(map(categories, (currTokType) => currTokType.CATEGORIES)));\n        const newCategories = difference(categories, result);\n        result = result.concat(newCategories);\n        if (isEmpty(newCategories)) {\n            searching = false;\n        }\n        else {\n            categories = newCategories;\n        }\n    }\n    return result;\n}\nexport function assignTokenDefaultProps(tokenTypes) {\n    forEach(tokenTypes, (currTokType) => {\n        if (!hasShortKeyProperty(currTokType)) {\n            tokenIdxToClass[tokenShortNameIdx] = currTokType;\n            currTokType.tokenTypeIdx = tokenShortNameIdx++;\n        }\n        // CATEGORIES? : TokenType | TokenType[]\n        if (hasCategoriesProperty(currTokType) &&\n            !isArray(currTokType.CATEGORIES)\n        // &&\n        // !isUndefined(currTokType.CATEGORIES.PATTERN)\n        ) {\n            currTokType.CATEGORIES = [currTokType.CATEGORIES];\n        }\n        if (!hasCategoriesProperty(currTokType)) {\n            currTokType.CATEGORIES = [];\n        }\n        if (!hasExtendingTokensTypesProperty(currTokType)) {\n            currTokType.categoryMatches = [];\n        }\n        if (!hasExtendingTokensTypesMapProperty(currTokType)) {\n            currTokType.categoryMatchesMap = {};\n        }\n    });\n}\nexport function assignCategoriesTokensProp(tokenTypes) {\n    forEach(tokenTypes, (currTokType) => {\n        // avoid duplications\n        currTokType.categoryMatches = [];\n        forEach(currTokType.categoryMatchesMap, (val, key) => {\n            currTokType.categoryMatches.push(tokenIdxToClass[key].tokenTypeIdx);\n        });\n    });\n}\nexport function assignCategoriesMapProp(tokenTypes) {\n    forEach(tokenTypes, (currTokType) => {\n        singleAssignCategoriesToksMap([], currTokType);\n    });\n}\nexport function singleAssignCategoriesToksMap(path, nextNode) {\n    forEach(path, (pathNode) => {\n        nextNode.categoryMatchesMap[pathNode.tokenTypeIdx] = true;\n    });\n    forEach(nextNode.CATEGORIES, (nextCategory) => {\n        const newPath = path.concat(nextNode);\n        // avoids infinite loops due to cyclic categories.\n        if (!includes(newPath, nextCategory)) {\n            singleAssignCategoriesToksMap(newPath, nextCategory);\n        }\n    });\n}\nexport function hasShortKeyProperty(tokType) {\n    return has(tokType, \"tokenTypeIdx\");\n}\nexport function hasCategoriesProperty(tokType) {\n    return has(tokType, \"CATEGORIES\");\n}\nexport function hasExtendingTokensTypesProperty(tokType) {\n    return has(tokType, \"categoryMatches\");\n}\nexport function hasExtendingTokensTypesMapProperty(tokType) {\n    return has(tokType, \"categoryMatchesMap\");\n}\nexport function isTokenType(tokType) {\n    return has(tokType, \"tokenTypeIdx\");\n}\n//# sourceMappingURL=tokens.js.map","import { has, isString, isUndefined } from \"lodash-es\";\nimport { Lexer } from \"./lexer_public.js\";\nimport { augmentTokenTypes, tokenStructuredMatcher } from \"./tokens.js\";\nexport function tokenLabel(tokType) {\n    if (hasTokenLabel(tokType)) {\n        return tokType.LABEL;\n    }\n    else {\n        return tokType.name;\n    }\n}\nexport function tokenName(tokType) {\n    return tokType.name;\n}\nexport function hasTokenLabel(obj) {\n    return isString(obj.LABEL) && obj.LABEL !== \"\";\n}\nconst PARENT = \"parent\";\nconst CATEGORIES = \"categories\";\nconst LABEL = \"label\";\nconst GROUP = \"group\";\nconst PUSH_MODE = \"push_mode\";\nconst POP_MODE = \"pop_mode\";\nconst LONGER_ALT = \"longer_alt\";\nconst LINE_BREAKS = \"line_breaks\";\nconst START_CHARS_HINT = \"start_chars_hint\";\nexport function createToken(config) {\n    return createTokenInternal(config);\n}\nfunction createTokenInternal(config) {\n    const pattern = config.pattern;\n    const tokenType = {};\n    tokenType.name = config.name;\n    if (!isUndefined(pattern)) {\n        tokenType.PATTERN = pattern;\n    }\n    if (has(config, PARENT)) {\n        throw (\"The parent property is no longer supported.\\n\" +\n            \"See: https://github.com/chevrotain/chevrotain/issues/564#issuecomment-349062346 for details.\");\n    }\n    if (has(config, CATEGORIES)) {\n        // casting to ANY as this will be fixed inside `augmentTokenTypes``\n        tokenType.CATEGORIES = config[CATEGORIES];\n    }\n    augmentTokenTypes([tokenType]);\n    if (has(config, LABEL)) {\n        tokenType.LABEL = config[LABEL];\n    }\n    if (has(config, GROUP)) {\n        tokenType.GROUP = config[GROUP];\n    }\n    if (has(config, POP_MODE)) {\n        tokenType.POP_MODE = config[POP_MODE];\n    }\n    if (has(config, PUSH_MODE)) {\n        tokenType.PUSH_MODE = config[PUSH_MODE];\n    }\n    if (has(config, LONGER_ALT)) {\n        tokenType.LONGER_ALT = config[LONGER_ALT];\n    }\n    if (has(config, LINE_BREAKS)) {\n        tokenType.LINE_BREAKS = config[LINE_BREAKS];\n    }\n    if (has(config, START_CHARS_HINT)) {\n        tokenType.START_CHARS_HINT = config[START_CHARS_HINT];\n    }\n    return tokenType;\n}\nexport const EOF = createToken({ name: \"EOF\", pattern: Lexer.NA });\naugmentTokenTypes([EOF]);\nexport function createTokenInstance(tokType, image, startOffset, endOffset, startLine, endLine, startColumn, endColumn) {\n    return {\n        image,\n        startOffset,\n        endOffset,\n        startLine,\n        endLine,\n        startColumn,\n        endColumn,\n        tokenTypeIdx: tokType.tokenTypeIdx,\n        tokenType: tokType,\n    };\n}\nexport function tokenMatcher(token, tokType) {\n    return tokenStructuredMatcher(token, tokType);\n}\n//# sourceMappingURL=tokens_public.js.map"],"names":["defaultLexerErrorProvider","buildUnableToPopLexerModeMessage","token","image","buildUnexpectedCharactersMessage","fullText","startOffset","length","line","column","charAt","regExpAstCache","regExpParser","getRegExpAst","regExp","regExpStr","toString","hasOwnProperty","regExpAst","pattern","clearRegExpParserCache","complementErrorMessage","failedOptimizationPrefixMsg","getOptimizedStartCodesIndices","ensureOptimizations","ast","firstChars","firstCharOptimizedIndices","value","flags","ignoreCase","e","message","msgSuffix","result","type","i","terms","term","atom","addOptimizedIdxToResult","complement","Error","forEach","code","range","rangeCode","from","to","minOptimizationVal","minUnOptVal","maxUnOptVal","minOptIdx","charCodeToOptimizedIndex","maxOptIdx","currOptIdx","isOptionalQuantifier","undefined","quantifier","atLeast","isWholeOptional","values","optimizedCharIdx","handleIgnoreCase","char","String","fromCharCode","upperChar","toUpperCase","charCodeAt","lowerChar","toLowerCase","findCode","setNode","targetCharCodes","find","codeOrRange","includes","targetCode","isArray","every","CharCodeFinder","constructor","super","this","found","visitChildren","node","visitLookahead","visitNegativeLookahead","visitCharacter","visitSet","canMatchCharCode","charCodes","RegExp","charCodeFinder","visit","PATTERN","DEFAULT_MODE","MODES","SUPPORT_STICKY","sticky","analyzeTokenTypes","tokenTypes","options","defaults","useSticky","debug","safeMode","positionTracking","lineTerminatorCharacters","tracer","msg","action","onlyRelevantTypes","initCharCodeToOptimizedIndexMap","reject","currType","Lexer","NA","allTransformedPatterns","patternIdxToType","patternIdxToGroup","patternIdxToLongerAltIdxArr","patternIdxToPushMode","patternIdxToPopMode","patternIdxToCanLineTerminator","patternIdxToIsCustom","patternIdxToShort","emptyGroups","patternIdxToConfig","hasCustom","map","currPattern","isRegExp","regExpSource","source","addStickyFlag","addStartOfInput","isFunction","exec","escapedRegExpString","replace","wrappedRegExp","tokenTypeIdx","clazz","groupName","GROUP","SKIPPED","isString","isUndefined","longerAltType","LONGER_ALT","longerAltIdxArr","indexOf","PUSH_MODE","has","lineTerminatorCharCodes","getCharCodes","tokType","LINE_BREAKS","checkLineBreaksIssues","isCustomPattern","isShortPattern","reduce","acc","x","idx","longerAlt","canLineTerminator","isCustom","short","group","push","pop","tokenType","canBeOptimized","charCodeToPatternIdxToConfig","currTokType","charCode","optimizedIdx","addToMapOfArrays","START_CHARS_HINT","lastOptimizedIdx","charOrInt","currOptimizedIdx","unicode","optimizedCodes","isEmpty","name","validatePatterns","validModesNames","errors","missingResult","findMissingPatterns","concat","invalidResult","findInvalidPatterns","valid","validTokenTypes","validateRegExpPattern","findInvalidGroupType","findModesThatDoNotExist","findUnreachablePatterns","withRegExpPatterns","filter","findEndOfInputAnchor","findStartOfInputAnchor","findUnsupportedFlags","findDuplicatePatterns","findEmptyMatchRegExps","tokenTypesWithMissingPattern","LexerDefinitionErrorType","MISSING_PATTERN","difference","tokenTypesWithInvalidPattern","INVALID_PATTERN","end_of_input","EndAnchorFinder","arguments","visitEndAnchor","invalidRegex","regexpAst","endAnchorVisitor","test","EOI_ANCHOR_FOUND","matchesEmptyString","EMPTY_MATCH_PATTERN","start_of_input","StartAnchorFinder","visitStartAnchor","startAnchorVisitor","SOI_ANCHOR_FOUND","invalidFlags","multiline","global","UNSUPPORTED_FLAGS_FOUND","identicalPatterns","outerType","innerType","compact","duplicatePatterns","currIdenticalSet","setOfIdentical","tokenTypeNames","dupPatternSrc","join","DUPLICATE_PATTERNS_FOUND","invalidTypes","INVALID_GROUP_TYPE_FOUND","validModes","invalidModes","PUSH_MODE_DOES_NOT_EXIST","canBeTested","str","noMetaChar","testIdx","testTokenType","UNREACHABLE_PATTERN","regExpArray","index","metaChars","performRuntimeChecks","lexerDefinition","trackLines","MULTI_MODE_LEXER_WITHOUT_DEFAULT_MODE","MULTI_MODE_LEXER_WITHOUT_MODES_PROPERTY","modes","defaultMode","MULTI_MODE_LEXER_DEFAULT_MODE_VALUE_DOES_NOT_EXIST","currModeValue","currModeName","currIdx","LEXER_DEFINITION_CANNOT_CONTAIN_UNDEFINED","currLongerAlt","MULTI_MODE_LEXER_LONGER_ALT_NOT_IN_CURRENT_MODE","performWarningRuntimeChecks","warnings","hasAnyLineBreak","allTokenTypes","flatten","concreteTokenTypes","terminatorCharCodes","currIssue","buildLineBreakIssueMessage","warningDescriptor","issue","NO_LINE_BREAKS_FLAGS","cloneEmptyGroups","clonedResult","groupKeys","keys","currKey","currGroupValue","LineTerminatorOptimizedTester","text","len","lastIndex","c","IDENTIFY_TERMINATOR","errMsg","CUSTOM_LINE_BREAK","details","charsOrCodes","numOrString","key","charCodeToOptimizedIdxMap","Array","DEFAULT_LEXER_CONFIG","deferDefinitionErrorsHandling","lineTerminatorsPattern","errorMessageProvider","traceInitPerf","skipValidations","recoveryEnabled","Object","freeze","config","lexerDefinitionErrors","lexerDefinitionWarning","trackStartLines","trackEndLines","canModeBeOptimized","TRACE_INIT","phaseDesc","phaseImpl","traceInitIndent","indent","traceInitMaxIdent","console","log","time","timer","traceMethod","warn","traceInitVal","Infinity","actualDefinition","hasOnlySingleMode","clone","allModeNames","currModDef","currModName","currAnalyzeResult","allErrMessages","error","allErrMessagesString","chopInput","identity","match","matchWithTest","updateLastIndex","noop","matchWithExec","handleModes","computeNewColumn","updateTokenEndLineColumnLocation","createTokenInstance","createFullToken","createStartOnlyToken","createOffsetOnlyToken","addToken","addTokenUsingPush","handlePayload","handlePayloadWithCustom","addTokenUsingMemberAccess","handlePayloadNoCustom","unOptimizedModes","cannotBeOptimized","modeName","tokenize","initialMode","tokenizeInternal","j","k","matchAltImage","matchedImage","payload","altPayload","imageLength","newToken","errLength","orgText","orgLength","offset","matchedTokensIndex","guessedNumberOfTokens","Math","floor","matchedTokens","groups","lineTerminatorPattern","currModePatternsLength","currCharCodeToPatternIdxToConfig","modeStack","emptyArray","getPossiblePatterns","getPossiblePatternsSlow","getPossiblePatternsOptimized","possiblePatterns","pop_mode","popToken","startLine","startColumn","newMode","last","modeCanBeOptimized","push_mode","currConfig","call","nextCharCode","chosenPatternIdxToConfig","chosenPatternsLength","singleCharCode","longerAltLength","longerAltConfig","longerAltPattern","foundTerminator","lastLTEndOffset","numOfLTsInMatch","errorStartOffset","errorLine","errorColumn","foundResyncPoint","tokens","pushMode","substring","newLastIndex","lastLTIdx","lastCharIsLT","fixForEndingInLT","endLine","endColumn","oldColumn","endOffset","tokenVector","tokenToAdd","tokenStructuredMatcher","tokInstance","tokConstructor","instanceType","isParent","categoryMatchesMap","tokenStructuredMatcherNoCategories","tokenShortNameIdx","tokenIdxToClass","augmentTokenTypes","tokenTypesAndParents","expandCategories","assignTokenDefaultProps","assignCategoriesMapProp","assignCategoriesTokensProp","categoryMatches","categories","searching","CATEGORIES","newCategories","hasShortKeyProperty","hasCategoriesProperty","hasExtendingTokensTypesProperty","hasExtendingTokensTypesMapProperty","val","singleAssignCategoriesToksMap","path","nextNode","pathNode","nextCategory","newPath","isTokenType","tokenLabel","hasTokenLabel","LABEL","obj","PARENT","POP_MODE","createToken","createTokenInternal","EOF","tokenMatcher"],"sourceRoot":""}